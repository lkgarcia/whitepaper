"use strict";(globalThis.webpackChunkai_whitepaper=globalThis.webpackChunkai_whitepaper||[]).push([[2453],{912:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/five-levels-of-autonomy-for-ai-agents-2e382dc7500c8810b796aa0b2f16aea1.png"},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var i=t(6540);const a={},s=i.createContext(a);function r(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),i.createElement(s.Provider,{value:n},e.children)}},9299:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"balancing-autonomy-and-agency","title":"Balancing Autonomy and Agency","description":"image-center","source":"@site/docs/02-balancing-autonomy-and-agency.md","sourceDirName":".","slug":"/balancing-autonomy-and-agency","permalink":"/whitepaper/balancing-autonomy-and-agency","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"banking","permalink":"/whitepaper/tags/banking"},{"inline":true,"label":"ai","permalink":"/whitepaper/tags/ai"},{"inline":true,"label":"agentic-ai","permalink":"/whitepaper/tags/agentic-ai"},{"inline":true,"label":"risk","permalink":"/whitepaper/tags/risk"}],"version":"current","lastUpdatedAt":1763534544000,"sidebarPosition":2,"frontMatter":{"title":"Balancing Autonomy and Agency","date":"2025-11-13T10:00:00.000Z","slug":"balancing-autonomy-and-agency","authors":["lkgarcia"],"tags":["banking","ai","agentic-ai","risk"],"prompt":"Title: Balancing Autonomy and Agency: Managing Emerging Risks in AI Agents\\n\\nMessage: Autonomous AI agents introduce unique operational, privacy, compliance, and governance risks. This paper outlines those risks, links them to technical design decisions, and offers practical guidance for safely deploying agentic systems in retail banking.\\n\\nPurpose: Produce a concise, structured, and actionable white paper that explains emerging risks related to agentic AI in banking and recommends governance, technical controls, and operational practices.\\n\\nGoals:\\n  - Define and distinguish the concepts of \\"agency\\" and \\"autonomy\\" and why both matter as risk dimensions.\\n  - Map common failure modes and threat scenarios to agent architectures and workflows.\\n  - Provide concrete governance, auditability, and human-in-the-loop patterns for banks.\\n  - Offer a realistic 3-point roadmap (short / mid / long term) for pilot \u2192 scale \u2192 govern.\\n  - Include illustrative examples and at least one step-by-step interaction (customer agent \u2194 service agent \u2194 compliance agent) with inputs, outputs, decision points, and escalation triggers.\\n\\nAudience: Technical product managers, architects, risk and compliance leads, and senior banking executives (mix of technical and non-technical readers).\\n\\nTone: Clear, authoritative, moderately technical; pragmatic and risk-aware with accessible explanations for non-technical stakeholders.\\n\\nLength & structure:\\n  - Target: ~1200\u20131600 words.\\n  - Use headings, short paragraphs, bullets (only when appropriate), and one illustrative diagram using Mermaid.\\n  - Include inline citations with footnotes in Markdown (e.g., [^2]) and links to source for claims and references.\\n    > Example:\\n    > `[^f]: [Okpala et al. (2024). _Agentic AI Systems Applied to Financial Services_ (arXiv preprint 2502.05439).](https://arxiv.org/abs/2502.05439)`\\n  - Do not bold headings.\\n\\nSections (required):\\n  - 1) Executive summary (100\u2013150 words)\\n  - 2) Introduction: context and scope (retail banking focus)\\n  - 3) Technical fundamentals: definitions (lean more towards practical operational implications for banks)\\n    - What is agency?\\n    - What is autonomy?\\n    - How do they relate to AI agents?\\n  - 4) Topics:\\n    - Levels of Autonomy in AI Agents (refer to source: arXiv:2506.12469)\\n    - Agency vs Autonomy as Risk Dimensions (risks from too much agency vs too much autonomy)\\n    - Banking regulation on AI agents (from news or include prediction from Gartner)\\n    - Real-life examples from news of incidents due to poor governance. (source from public news in the past 2 years for incidents involving AI governance failures)\\n  - 5) Recommendations and 3-point roadmap (short-, mid-, long-term) -- (assume bank that is starting from scratch on AI agents):\\n    - Balancing the Two Levers (agency and autonomy)\\n    - Risk analysis and failure modes: examples mapped to architecture/components\\n    - Step-by-step illustrative scenario: customer agent \u2192 service agent \u2192 compliance agent (include inputs, outputs, decision points, and human escalation rules)\\n  - 6) Conclusion\\n\\nExamples & requirements:\\n  - Focus on retail banking use cases (payments, account servicing, dispute handling, fraud monitoring).\\n\\nConstraints: Avoid vendor promotion, avoid unrealistic timelines, and qualify speculative claims. Emphasize data protection, privacy-by-design, auditability, explainability, and human-in-the-loop controls for high-risk decisions.\\n\\nDeep Research clarifications:\\n  - Tone for both senior stakeholders and technical depth for product/risk teams\\n  - roadmap to be geared specifically toward large incumbent banks balancing technology scaling or regulatory alignment\\n  - For the Mermaid diagram: add both process flow (e.g., agent escalation) and system architecture (e.g., control layers)\\n  - Do you want the white paper to include references to specific bank names or should all examples remain anonymized unless widely publicized? Anwer: anonymized.\\n  - Should the roadmap focus on a particular jurisdiction\'s regulatory environment (e.g., US, EU, UK), or be jurisdiction-neutral? Answer: jurisdiction-neutral.\\n  - Do you have any preferences for the types of agentic systems in scope (e.g., customer-facing chatbots, internal fraud monitoring agents, generative agents for servicing)? Answer: mix of customer-facing and internal agents.\\n  - Like me to include citations to specific research sources from arXiv and analyst reports (e.g., Gartner), or should those be kept general unless highly relevant? Answer: include specific citations where relevant.\\n  - Like me to include a downloadable diagram image alongside the Mermaid syntax, or just the Mermaid code block? Answer: just the Mermaid code block.\\n  - Like me to prioritize any specific agentic workflows (e.g., payment dispute resolution, onboarding KYC automation, fraud detection, etc.) in the scenario and risk mapping? Answer: payment dispute resolution.\\n  - Like me to include real-world news citations about AI governance failures from specific banks or tech vendors, or keep those anonymized as well unless already widely publicized? Answer: anonymized unless widely publicized.\\n\\nSources:\\n  - ArXiv preprints and peer-reviewed research\\n  - Analyst reports (e.g., Gartner)\\n  - Academic publications from well-known universities and researchers\\n\\nOutput Format:\\n  - Downloadable Markdown file.\\n"},"sidebar":"docsSidebar","previous":{"title":"Banking Reimagined Through Agentic AI","permalink":"/whitepaper/banking-reimagined-through-agentic-ai"},"next":{"title":"The AI Use Case Canvas","permalink":"/whitepaper/the-ai-use-case-canvas"}}');var a=t(4848),s=t(8453);const r={title:"Balancing Autonomy and Agency",date:new Date("2025-11-13T10:00:00.000Z"),slug:"balancing-autonomy-and-agency",authors:["lkgarcia"],tags:["banking","ai","agentic-ai","risk"],prompt:'Title: Balancing Autonomy and Agency: Managing Emerging Risks in AI Agents\n\nMessage: Autonomous AI agents introduce unique operational, privacy, compliance, and governance risks. This paper outlines those risks, links them to technical design decisions, and offers practical guidance for safely deploying agentic systems in retail banking.\n\nPurpose: Produce a concise, structured, and actionable white paper that explains emerging risks related to agentic AI in banking and recommends governance, technical controls, and operational practices.\n\nGoals:\n  - Define and distinguish the concepts of "agency" and "autonomy" and why both matter as risk dimensions.\n  - Map common failure modes and threat scenarios to agent architectures and workflows.\n  - Provide concrete governance, auditability, and human-in-the-loop patterns for banks.\n  - Offer a realistic 3-point roadmap (short / mid / long term) for pilot \u2192 scale \u2192 govern.\n  - Include illustrative examples and at least one step-by-step interaction (customer agent \u2194 service agent \u2194 compliance agent) with inputs, outputs, decision points, and escalation triggers.\n\nAudience: Technical product managers, architects, risk and compliance leads, and senior banking executives (mix of technical and non-technical readers).\n\nTone: Clear, authoritative, moderately technical; pragmatic and risk-aware with accessible explanations for non-technical stakeholders.\n\nLength & structure:\n  - Target: ~1200\u20131600 words.\n  - Use headings, short paragraphs, bullets (only when appropriate), and one illustrative diagram using Mermaid.\n  - Include inline citations with footnotes in Markdown (e.g., [^2]) and links to source for claims and references.\n    > Example:\n    > `[^f]: [Okpala et al. (2024). _Agentic AI Systems Applied to Financial Services_ (arXiv preprint 2502.05439).](https://arxiv.org/abs/2502.05439)`\n  - Do not bold headings.\n\nSections (required):\n  - 1) Executive summary (100\u2013150 words)\n  - 2) Introduction: context and scope (retail banking focus)\n  - 3) Technical fundamentals: definitions (lean more towards practical operational implications for banks)\n    - What is agency?\n    - What is autonomy?\n    - How do they relate to AI agents?\n  - 4) Topics:\n    - Levels of Autonomy in AI Agents (refer to source: arXiv:2506.12469)\n    - Agency vs Autonomy as Risk Dimensions (risks from too much agency vs too much autonomy)\n    - Banking regulation on AI agents (from news or include prediction from Gartner)\n    - Real-life examples from news of incidents due to poor governance. (source from public news in the past 2 years for incidents involving AI governance failures)\n  - 5) Recommendations and 3-point roadmap (short-, mid-, long-term) -- (assume bank that is starting from scratch on AI agents):\n    - Balancing the Two Levers (agency and autonomy)\n    - Risk analysis and failure modes: examples mapped to architecture/components\n    - Step-by-step illustrative scenario: customer agent \u2192 service agent \u2192 compliance agent (include inputs, outputs, decision points, and human escalation rules)\n  - 6) Conclusion\n\nExamples & requirements:\n  - Focus on retail banking use cases (payments, account servicing, dispute handling, fraud monitoring).\n\nConstraints: Avoid vendor promotion, avoid unrealistic timelines, and qualify speculative claims. Emphasize data protection, privacy-by-design, auditability, explainability, and human-in-the-loop controls for high-risk decisions.\n\nDeep Research clarifications:\n  - Tone for both senior stakeholders and technical depth for product/risk teams\n  - roadmap to be geared specifically toward large incumbent banks balancing technology scaling or regulatory alignment\n  - For the Mermaid diagram: add both process flow (e.g., agent escalation) and system architecture (e.g., control layers)\n  - Do you want the white paper to include references to specific bank names or should all examples remain anonymized unless widely publicized? Anwer: anonymized.\n  - Should the roadmap focus on a particular jurisdiction\'s regulatory environment (e.g., US, EU, UK), or be jurisdiction-neutral? Answer: jurisdiction-neutral.\n  - Do you have any preferences for the types of agentic systems in scope (e.g., customer-facing chatbots, internal fraud monitoring agents, generative agents for servicing)? Answer: mix of customer-facing and internal agents.\n  - Like me to include citations to specific research sources from arXiv and analyst reports (e.g., Gartner), or should those be kept general unless highly relevant? Answer: include specific citations where relevant.\n  - Like me to include a downloadable diagram image alongside the Mermaid syntax, or just the Mermaid code block? Answer: just the Mermaid code block.\n  - Like me to prioritize any specific agentic workflows (e.g., payment dispute resolution, onboarding KYC automation, fraud detection, etc.) in the scenario and risk mapping? Answer: payment dispute resolution.\n  - Like me to include real-world news citations about AI governance failures from specific banks or tech vendors, or keep those anonymized as well unless already widely publicized? Answer: anonymized unless widely publicized.\n\nSources:\n  - ArXiv preprints and peer-reviewed research\n  - Analyst reports (e.g., Gartner)\n  - Academic publications from well-known universities and researchers\n\nOutput Format:\n  - Downloadable Markdown file.\n'},o="Balancing Autonomy and Agency: Managing Emerging Risks in AI Agents",l={},c=[{value:"Executive Summary",id:"executive-summary",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Core Concepts: Agency and Autonomy in AI Agents",id:"core-concepts-agency-and-autonomy-in-ai-agents",level:2},{value:"What is Agency?",id:"what-is-agency",level:3},{value:"What is Autonomy?",id:"what-is-autonomy",level:3},{value:"Levels of Autonomy in AI Agents",id:"levels-of-autonomy-in-ai-agents",level:2},{value:"Differentiating Risk: Agency vs. Autonomy",id:"differentiating-risk-agency-vs-autonomy",level:2},{value:"High Agency",id:"high-agency",level:3},{value:"Risk Categories",id:"risk-categories",level:4},{value:"High Autonomy",id:"high-autonomy",level:3},{value:"Risk Categories",id:"risk-categories-1",level:4},{value:"Banking Regulation and AI Governance Landscape",id:"banking-regulation-and-ai-governance-landscape",level:2},{value:"Real-World Examples of AI Governance Failures",id:"real-world-examples-of-ai-governance-failures",level:2},{value:"Recommendations and Roadmap for Safe Agent Adoption",id:"recommendations-and-roadmap-for-safe-agent-adoption",level:2},{value:"Balancing the Two Levers: Design Principles",id:"balancing-the-two-levers-design-principles",level:3},{value:"1. Limit agency for high-stakes tasks",id:"1-limit-agency-for-high-stakes-tasks",level:4},{value:"2. Calibrate autonomy to maturity and risk",id:"2-calibrate-autonomy-to-maturity-and-risk",level:4},{value:"3. Add human guardrails at critical points",id:"3-add-human-guardrails-at-critical-points",level:4},{value:"4. Compartmentalize roles with multi-agent design",id:"4-compartmentalize-roles-with-multi-agent-design",level:4},{value:"5. Enable manual override and fallback plans",id:"5-enable-manual-override-and-fallback-plans",level:4},{value:"Risk Analysis and Failure Mode Mitigations",id:"risk-analysis-and-failure-mode-mitigations",level:3},{value:"Illustrative Scenario: Payment Dispute Resolution Workflow",id:"illustrative-scenario-payment-dispute-resolution-workflow",level:3},{value:"Three-Point Roadmap: Pilot, Scale, Govern",id:"three-point-roadmap-pilot-scale-govern",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",br:"br",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",img:"img",li:"li",mermaid:"mermaid",ol:"ol",p:"p",section:"section",strong:"strong",sup:"sup",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"balancing-autonomy-and-agency-managing-emerging-risks-in-ai-agents",children:"Balancing Autonomy and Agency: Managing Emerging Risks in AI Agents"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"image-center",src:t(9813).A+"",width:"1536",height:"1024"})}),"\n",(0,a.jsx)(n.h2,{id:"executive-summary",children:"Executive Summary"}),"\n",(0,a.jsxs)(n.p,{children:["Autonomous AI agents offer major efficiency gains but introduce new risk dimensions. This paper defines two critical factors\u2014",(0,a.jsx)(n.strong,{children:"agency"})," (decision-making power) and ",(0,a.jsx)(n.strong,{children:"autonomy"})," (independence from human oversight)\u2014and explains why balancing them is essential. We map common failure modes to agent architectures and provide a practical roadmap for banks to pilot, scale, and govern agentic AI safely. Key controls include auditability, human-in-the-loop checkpoints, and constrained tool access. A step-by-step scenario illustrates how agents can coordinate safely in high-stakes workflows like payment disputes. With thoughtful design and proactive governance, banks can unlock agentic AI\u2019s value while managing its risks."]}),"\n",(0,a.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsxs)(n.p,{children:["In the previous paper in this series, ",(0,a.jsx)(n.em,{children:"Banking Reimagined Through Agentic AI"}),", we explored the next evolution of artificial intelligence\u2014Agentic AI\u2014and its potential to transform banking operations and customer service by enabling AI agents to act on behalf of customers and employees."]}),"\n",(0,a.jsxs)(n.p,{children:["By 2025, nearly half of banks had created \u201cAI supervisor\u201d roles, reflecting rapid adoption of agentic AI. Common use cases include customer-facing chatbots (75% of banks), fraud detection agents (two-thirds), and internal digital assistants for loans or IT",(0,a.jsx)(n.sup,{children:(0,a.jsx)(n.a,{href:"#user-content-fn-a",id:"user-content-fnref-a","data-footnote-ref":!0,"aria-describedby":"footnote-label",children:"1"})}),". While the potential value is estimated at ",(0,a.jsx)(n.strong,{children:"$450 billion"}),", these agents introduce ",(0,a.jsx)(n.strong,{children:"unique risks"}),"\u2014unlike traditional software, they make complex decisions, adapt behaviors, and interact across systems in often ",(0,a.jsx)(n.strong,{children:"unpredictable"})," ways."]}),"\n",(0,a.jsxs)(n.p,{children:["Regulators and executives are taking notice. In 2024, the U.S. Consumer Financial Protection Bureau warned that poorly governed banking chatbots risk compliance violations by mishandling disputes or giving incorrect information",(0,a.jsx)(n.sup,{children:(0,a.jsx)(n.a,{href:"#user-content-fn-b",id:"user-content-fnref-b","data-footnote-ref":!0,"aria-describedby":"footnote-label",children:"2"})}),". Globally, oversight expectations are rising. Similarly, the EU\u2019s upcoming AI Act classifies many financial AI systems as \u201chigh risk,\u201d requiring strict controls on privacy, fairness, and human accountability."]}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsxs)(n.p,{children:["\xa0",(0,a.jsx)(n.br,{}),"\n",(0,a.jsxs)(n.em,{children:["Singapore\u2019s central bank (MAS) proposed holding boards directly accountable for AI failures, warning that ",(0,a.jsx)(n.strong,{children:"\u201cAI agents with greater autonomy and tool access could amplify risks\u201d"})," if not properly governed."]}),(0,a.jsx)(n.br,{}),"\n","\u2014 CIO.com (2025)",(0,a.jsx)(n.sup,{children:(0,a.jsx)(n.a,{href:"#user-content-fn-c",id:"user-content-fnref-c","data-footnote-ref":!0,"aria-describedby":"footnote-label",children:"3"})}),(0,a.jsx)(n.br,{}),"\n","\xa0"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["This white paper focuses on ",(0,a.jsx)(n.strong,{children:"banking"})," use cases and the emerging risks of AI agents. We explore how ",(0,a.jsx)(n.strong,{children:"agency"})," and ",(0,a.jsx)(n.strong,{children:"autonomy"})," enable powerful capabilities but also introduce new failure modes",(0,a.jsx)(n.sup,{children:(0,a.jsx)(n.a,{href:"#user-content-fn-d",id:"user-content-fnref-d","data-footnote-ref":!0,"aria-describedby":"footnote-label",children:"4"})}),". We offer practical guidance\u2014from design choices (e.g. limiting autonomy) to governance practices (e.g. auditability, human oversight)\u2014and present a three-phase roadmap (pilot \u2192 scale \u2192 govern) to help banks deploy agentic AI responsibly, driving innovation ",(0,a.jsx)(n.strong,{children:"without"})," compromising compliance, security, or trust."]}),"\n",(0,a.jsx)(n.h2,{id:"core-concepts-agency-and-autonomy-in-ai-agents",children:"Core Concepts: Agency and Autonomy in AI Agents"}),"\n",(0,a.jsxs)(n.p,{children:["To assess risk, it's critical to define ",(0,a.jsx)(n.strong,{children:"agency"})," and ",(0,a.jsx)(n.strong,{children:"autonomy"})," in AI systems and understand their roles in agent behavior."]}),"\n",(0,a.jsx)(n.h3,{id:"what-is-agency",children:"What is Agency?"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.em,{children:"Agency"})," refers to an AI\u2019s capacity to act purposefully, make decisions, and influence its environment on behalf of users."]}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsxs)(n.p,{children:["\xa0",(0,a.jsx)(n.br,{}),"\n","The ",(0,a.jsx)(n.em,{children:"power"})," or ",(0,a.jsx)(n.em,{children:"capability"})," of the agent \u2014 what it can do (e.g., tools, APIs, systems access).",(0,a.jsx)(n.br,{}),"\n","\xa0"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["Unlike rule-based bots, agentic systems can interpret goals, take initiative, and adapt strategies\u2014even in novel situations",(0,a.jsx)(n.sup,{children:(0,a.jsx)(n.a,{href:"#user-content-fn-e",id:"user-content-fnref-e","data-footnote-ref":!0,"aria-describedby":"footnote-label",children:"5"})}),". For example, a high-agency service agent might analyze financial data and initiate loan processing, while a low-agency one simply retrieves information. Greater agency enables flexibility but increases risk if misaligned with intent or policy."]}),"\n",(0,a.jsx)(n.h3,{id:"what-is-autonomy",children:"What is Autonomy?"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.em,{children:"Autonomy"})," measures how independently an agent operates without human input."]}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsxs)(n.p,{children:["\xa0",(0,a.jsx)(n.br,{}),"\n","The ",(0,a.jsx)(n.em,{children:"freedom"})," of the agent \u2014 how independently it acts without supervision.",(0,a.jsx)(n.br,{}),"\n","\xa0"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"It reflects the level of oversight built into its workflow\u2014from fully supervised to end-to-end execution. Most bank use cases today adopt low to moderate autonomy to preserve control in high-risk scenarios. For instance, a chatbot may handle basic queries autonomously but escalate fraud concerns to a human."}),"\n",(0,a.jsxs)(n.p,{children:["Agency and autonomy are distinct ",(0,a.jsx)(n.strong,{children:"governance levers"}),". An agent may have low agency (e.g., limited tools) but high autonomy (e.g., runs unsupervised), or vice versa. Seeking approval signals limited autonomy; modifying systems reflects greater agency. Disentangling the two helps tailor oversight to the nature and risk of the task."]}),"\n",(0,a.jsx)(n.h2,{id:"levels-of-autonomy-in-ai-agents",children:"Levels of Autonomy in AI Agents"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.img,{alt:"image-center",src:t(912).A+"",width:"2408",height:"778"}),"\n",(0,a.jsxs)("p",{class:"center",children:[" ",(0,a.jsx)(n.em,{children:"Figure 1: Five Levels of Autonomy for AI Agents"})," ",(0,a.jsx)(n.sup,{children:(0,a.jsx)(n.a,{href:"#user-content-fn-d",id:"user-content-fnref-d-2","data-footnote-ref":!0,"aria-describedby":"footnote-label",children:"4"})})," "]})]}),"\n",(0,a.jsxs)(n.p,{children:["Autonomy in AI agents exists on a spectrum, not as an all-or-nothing property. A five-level framework\u2014ranging from ",(0,a.jsx)(n.strong,{children:"Operator"})," to ",(0,a.jsx)(n.strong,{children:"Observer"}),"\u2014is commonly used to describe how much independence an agent has in decision-making and execution",(0,a.jsx)(n.sup,{children:(0,a.jsx)(n.a,{href:"#user-content-fn-d",id:"user-content-fnref-d-3","data-footnote-ref":!0,"aria-describedby":"footnote-label",children:"4"})}),". This structure clarifies the balance of control between human and AI across different use cases:"]}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{style:{textAlign:"left"},children:(0,a.jsx)(n.strong,{children:"Level"})}),(0,a.jsx)(n.th,{style:{textAlign:"left"},children:(0,a.jsx)(n.strong,{children:"Role"})}),(0,a.jsx)(n.th,{style:{textAlign:"left"},children:(0,a.jsx)(n.strong,{children:"Description"})}),(0,a.jsx)(n.th,{style:{textAlign:"left"},children:(0,a.jsx)(n.strong,{children:"Example"})})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{style:{textAlign:"left"},children:(0,a.jsx)(n.strong,{children:"1"})}),(0,a.jsx)(n.td,{style:{textAlign:"left"},children:"Operator"}),(0,a.jsx)(n.td,{style:{textAlign:"left"},children:"AI acts only when explicitly instructed by a human."}),(0,a.jsx)(n.td,{style:{textAlign:"left"},children:"A task bot triggered manually to retrieve reports."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{style:{textAlign:"left"},children:(0,a.jsx)(n.strong,{children:"2"})}),(0,a.jsx)(n.td,{style:{textAlign:"left"},children:"Collaborator"}),(0,a.jsx)(n.td,{style:{textAlign:"left"},children:"AI assists users but requires frequent guidance or intervention."}),(0,a.jsx)(n.td,{style:{textAlign:"left"},children:"A chatbot that drafts replies but needs staff approval to send them."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{style:{textAlign:"left"},children:(0,a.jsx)(n.strong,{children:"3"})}),(0,a.jsx)(n.td,{style:{textAlign:"left"},children:"Consultant"}),(0,a.jsx)(n.td,{style:{textAlign:"left"},children:"AI performs defined tasks with some independence, deferring major decisions to humans."}),(0,a.jsx)(n.td,{style:{textAlign:"left"},children:"A credit risk agent that recommends approvals reviewed by an underwriter."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{style:{textAlign:"left"},children:(0,a.jsx)(n.strong,{children:"4"})}),(0,a.jsx)(n.td,{style:{textAlign:"left"},children:"Approver"}),(0,a.jsx)(n.td,{style:{textAlign:"left"},children:"AI operates independently in routine tasks, escalating exceptions to humans."}),(0,a.jsx)(n.td,{style:{textAlign:"left"},children:"A fraud system that blocks common cases but flags unusual ones."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{style:{textAlign:"left"},children:(0,a.jsx)(n.strong,{children:"5"})}),(0,a.jsx)(n.td,{style:{textAlign:"left"},children:"Observer"}),(0,a.jsx)(n.td,{style:{textAlign:"left"},children:"AI functions autonomously end-to-end, with little to no human involvement."}),(0,a.jsx)(n.td,{style:{textAlign:"left"},children:"An IT monitoring agent that restarts servers without human input."})]})]})]}),"\n",(0,a.jsxs)("p",{class:"center",children:[" ",(0,a.jsx)(n.em,{children:"Table 1: Five Levels of Autonomy for AI Agents"})," "]}),"\n",(0,a.jsx)(n.p,{children:"In real-world deployments, an agent's autonomy may vary by task or context. For instance, a customer service agent might autonomously answer basic queries but escalate sensitive topics to a human. Many agent designs incorporate such dynamic shifts in autonomy to match risk levels and regulatory expectations."}),"\n",(0,a.jsxs)(n.p,{children:["This tiered approach also aligns with emerging concepts like ",(0,a.jsx)(n.strong,{children:"AI autonomy certification"}),", which may eventually help institutions communicate an agent's oversight level more transparently",(0,a.jsx)(n.sup,{children:(0,a.jsx)(n.a,{href:"#user-content-fn-d",id:"user-content-fnref-d-4","data-footnote-ref":!0,"aria-describedby":"footnote-label",children:"4"})}),". While formal standards are still evolving, defining and documenting each agent\u2019s autonomy level can support clearer governance, risk assessment, and stakeholder alignment."]}),"\n",(0,a.jsx)(n.h2,{id:"differentiating-risk-agency-vs-autonomy",children:"Differentiating Risk: Agency vs. Autonomy"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Agency"})," and ",(0,a.jsx)(n.strong,{children:"autonomy"})," introduce distinct risk types in AI systems."]}),"\n",(0,a.jsx)(n.h3,{id:"high-agency",children:"High Agency"}),"\n",(0,a.jsxs)(n.p,{children:["When an agent has ",(0,a.jsx)(n.em,{children:"too much capability or access"}),", risks are primarily ",(0,a.jsx)(n.strong,{children:"impact-based"})," \u2014 i.e., what damage it could cause ",(0,a.jsx)(n.em,{children:"if it acts incorrectly"}),"."]}),"\n",(0,a.jsx)(n.admonition,{title:"Key Governance Lever",type:"tip",children:(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.em,{children:"Scope control"})," \u2014 limit the agent\u2019s reach and abilities."]})}),"\n",(0,a.jsx)(n.p,{children:"Agentic systems may make policy-like decisions that introduce bias, misalign with goals, or reduce explainability, complicating audits and customer resolution."}),"\n",(0,a.jsx)(n.admonition,{title:"Example Scenarios",type:"note",children:(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Agent can execute API calls across financial systems."}),"\n",(0,a.jsx)(n.li,{children:"Agent modifies data, triggers transactions, or reconfigures settings."}),"\n",(0,a.jsx)(n.li,{children:"Agent sends external communications (emails, posts, messages)."}),"\n"]})}),"\n",(0,a.jsx)(n.h4,{id:"risk-categories",children:"Risk Categories"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:(0,a.jsx)(n.strong,{children:"Risk Type"})}),(0,a.jsx)(n.th,{children:(0,a.jsx)(n.strong,{children:"Description"})}),(0,a.jsx)(n.th,{children:(0,a.jsx)(n.strong,{children:"Example"})})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Operational Risk"})}),(0,a.jsx)(n.td,{children:"Excessive or incorrect actions cause service disruption or financial loss."}),(0,a.jsx)(n.td,{children:"Agent approves invalid transactions."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Security Risk"})}),(0,a.jsx)(n.td,{children:"Unauthorized tool use or privileged access abuse."}),(0,a.jsx)(n.td,{children:"Agent calls admin APIs beyond scope."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Reputational Risk"})}),(0,a.jsx)(n.td,{children:"Public-facing actions without validation."}),(0,a.jsx)(n.td,{children:"Agent posts unverified or inappropriate content."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Compliance Risk"})}),(0,a.jsx)(n.td,{children:"Violates policy or legal requirements."}),(0,a.jsx)(n.td,{children:"Agent mishandles personal data."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Cascade Risk"})}),(0,a.jsx)(n.td,{children:"Tool chaining triggers unintended downstream effects."}),(0,a.jsx)(n.td,{children:"Agent runs a script that impacts multiple systems."})]})]})]}),"\n",(0,a.jsxs)("p",{class:"center",children:[" ",(0,a.jsx)(n.em,{children:"Table 2: High Agency Risk Categories"})," ",(0,a.jsx)(n.sup,{children:(0,a.jsx)(n.a,{href:"#user-content-fn-d",id:"user-content-fnref-d-5","data-footnote-ref":!0,"aria-describedby":"footnote-label",children:"4"})})," "]}),"\n",(0,a.jsx)(n.admonition,{title:"Analogy",type:"info",children:(0,a.jsx)(n.p,{children:"Like giving an intern unrestricted access \u2014 well-intentioned, but risky at scale."})}),"\n",(0,a.jsx)(n.h3,{id:"high-autonomy",children:"High Autonomy"}),"\n",(0,a.jsxs)(n.p,{children:["When an agent operates ",(0,a.jsx)(n.em,{children:"too independently"})," (without oversight or feedback), risks are primarily ",(0,a.jsx)(n.strong,{children:"process-based"})," \u2014 i.e., when, how, and under what conditions it acts."]}),"\n",(0,a.jsx)(n.admonition,{title:"Key Governance Lever",type:"tip",children:(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.em,{children:"Oversight control"})," \u2014 limit how long or how freely it runs without human input."]})}),"\n",(0,a.jsxs)(n.p,{children:["Unchecked behavior can result in ",(0,a.jsx)(n.strong,{children:"disruptions"}),", ",(0,a.jsx)(n.strong,{children:"security breaches"}),", or customer harm \u2014 such as chatbots giving false information or exposing sensitive data."]}),"\n",(0,a.jsx)(n.admonition,{title:"Example Scenarios",type:"note",children:(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Agent runs continuously without checkpoints."}),"\n",(0,a.jsx)(n.li,{children:"Agent self-initiates actions or escalations."}),"\n",(0,a.jsx)(n.li,{children:"Agent adapts policies without validation."}),"\n"]})}),"\n",(0,a.jsx)(n.h4,{id:"risk-categories-1",children:"Risk Categories"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:(0,a.jsx)(n.strong,{children:"Risk Type"})}),(0,a.jsx)(n.th,{children:(0,a.jsx)(n.strong,{children:"Description"})}),(0,a.jsx)(n.th,{children:(0,a.jsx)(n.strong,{children:"Example"})})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Control Risk"})}),(0,a.jsx)(n.td,{children:"Lack of human oversight or auditability."}),(0,a.jsx)(n.td,{children:"Agent acts with no approval history."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Drift Risk"})}),(0,a.jsx)(n.td,{children:"Behavioral deviation over time."}),(0,a.jsx)(n.td,{children:"Agent \u201clearns\u201d undesirable patterns or bias."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Ethical Risk"})}),(0,a.jsx)(n.td,{children:"Misaligned decisions or fairness violations."}),(0,a.jsx)(n.td,{children:"Agent denies service using biased logic."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Accountability Risk"})}),(0,a.jsx)(n.td,{children:"Unclear responsibility for outcomes."}),(0,a.jsx)(n.td,{children:"Who is liable if the agent fails silently?"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Runaway Risk"})}),(0,a.jsx)(n.td,{children:"Recurring loops or actions without termination."}),(0,a.jsx)(n.td,{children:"Agent retries endlessly or spams actions."})]})]})]}),"\n",(0,a.jsxs)("p",{class:"center",children:[" ",(0,a.jsx)(n.em,{children:"Table 3: High Autonomy Risk Categories"})," "]}),"\n",(0,a.jsx)(n.admonition,{title:"Analogy",type:"info",children:(0,a.jsx)(n.p,{children:"Like a self-driving car told to \u201ckeep going\u201d \u2014 even when conditions change."})}),"\n",(0,a.jsxs)(n.p,{children:["Agency and autonomy often interact, but their risks are distinct: ",(0,a.jsx)(n.strong,{children:"excess autonomy"})," drives operational failures, while ",(0,a.jsx)(n.strong,{children:"excess agency"})," introduces compliance and strategic exposure. Even low-agency systems can cause harm if left unsupervised, while high-agency systems must be closely governed. Effective AI design requires balancing both based on task sensitivity and organizational risk appetite."]}),"\n",(0,a.jsx)(n.h2,{id:"banking-regulation-and-ai-governance-landscape",children:"Banking Regulation and AI Governance Landscape"}),"\n",(0,a.jsxs)(n.p,{children:["While no unified regulation governs AI agents, global regulators are increasingly setting expectations that address both ",(0,a.jsx)(n.strong,{children:"agency"})," and ",(0,a.jsx)(n.strong,{children:"autonomy"})," in AI systems."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Human accountability"})," is a central theme. In 2025, Singapore\u2019s MAS proposed that boards attest to their understanding of deployed AI, warning that greater autonomy could amplify risks like service failures or missed financial crimes",(0,a.jsx)(n.sup,{children:(0,a.jsx)(n.a,{href:"#user-content-fn-c",id:"user-content-fnref-c-2","data-footnote-ref":!0,"aria-describedby":"footnote-label",children:"3"})}),". The EU\u2019s draft AI Act classifies credit scoring and fraud detection as \u201chigh risk,\u201d requiring human-in-the-loop controls, transparency, and bias testing. In the U.S., existing rules\u2014such as fair lending laws and model risk guidance (SR 11-7)\u2014are being applied to AI, requiring validation, documentation, and monitoring."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Operational risk and consumer protection"})," are also key. The U.S. CFPB cautioned in 2023 that chatbots must not obstruct customer access to resolution pathways",(0,a.jsx)(n.sup,{children:(0,a.jsx)(n.a,{href:"#user-content-fn-b",id:"user-content-fnref-b-2","data-footnote-ref":!0,"aria-describedby":"footnote-label",children:"2"})}),". In the UK, the FCA and Bank of England noted that while most firms use AI, many lack a full understanding of their systems, raising concerns about fairness, security, and explainability",(0,a.jsx)(n.sup,{children:(0,a.jsx)(n.a,{href:"#user-content-fn-g",id:"user-content-fnref-g","data-footnote-ref":!0,"aria-describedby":"footnote-label",children:"6"})}),"."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Documentation and auditability"})," are emerging priorities. Regulators expect AI agents\u2019 actions and decisions to be logged\u2014particularly in multi-agent workflows. Some banks have begun developing internal ",(0,a.jsx)(n.strong,{children:"AI registers"})," to track models, usage, data inputs, and responsible owners. These may soon become mandatory under laws like the EU AI Act."]}),"\n",(0,a.jsxs)(n.p,{children:["Lastly, regulators are signaling that ",(0,a.jsx)(n.strong,{children:"third-party AI services"})," fall under the same accountability standards. Banks must apply governance controls not only to internal agents but also to external vendors\u2014extending familiar cloud oversight practices to AI. The core message: ",(0,a.jsx)(n.em,{children:"banks remain fully responsible for the outcomes of their AI systems"}),", regardless of who builds them."]}),"\n",(0,a.jsx)(n.h2,{id:"real-world-examples-of-ai-governance-failures",children:"Real-World Examples of AI Governance Failures"}),"\n",(0,a.jsx)(n.p,{children:"Though fully autonomous agents in banking are still emerging, early incidents already highlight governance gaps:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Credit Bias and Litigation:"})," Some banks faced lawsuits after AI credit models disproportionately denied or overcharged minority applicants",(0,a.jsx)(n.sup,{children:(0,a.jsx)(n.a,{href:"#user-content-fn-f",id:"user-content-fnref-f","data-footnote-ref":!0,"aria-describedby":"footnote-label",children:"7"})}),". The issue often stemmed from biased training data and unchecked optimization goals\u2014high agency without ethical constraints."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Chatbot Escalation Failures:"})," In 2023, regulators received complaints about chatbots mishandling disputes and failing to escalate to humans",(0,a.jsx)(n.sup,{children:(0,a.jsx)(n.a,{href:"#user-content-fn-b",id:"user-content-fnref-b-3","data-footnote-ref":!0,"aria-describedby":"footnote-label",children:"2"})}),". One bot confirmed a dispute and promised a refund\u2014but never triggered any backend process, violating resolution timelines."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Automation Glitches:"})," In 2024, a bank\u2019s automated system mistakenly showed $0 balances to 20,000 customers, causing panic",(0,a.jsx)(n.sup,{children:(0,a.jsx)(n.a,{href:"#user-content-fn-h",id:"user-content-fnref-h","data-footnote-ref":!0,"aria-describedby":"footnote-label",children:"8"})}),". While not AI-driven, the incident illustrates how unmonitored automation can scale errors instantly."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Data Leakage Risks:"})," Employees using external AI tools exposed confidential client data",(0,a.jsx)(n.sup,{children:(0,a.jsx)(n.a,{href:"#user-content-fn-i",id:"user-content-fnref-i","data-footnote-ref":!0,"aria-describedby":"footnote-label",children:"9"})}),", prompting many banks to restrict public chatbot use until secure alternatives were deployed."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["These failures weren\u2019t caused by rogue AI\u2014they stemmed from ",(0,a.jsx)(n.strong,{children:"routine breakdowns in oversight, escalation, and testing"}),". Each scenario underscores the need for guardrails, auditability, and clear accountability\u2014issues addressed in the next section."]}),"\n",(0,a.jsx)(n.h2,{id:"recommendations-and-roadmap-for-safe-agent-adoption",children:"Recommendations and Roadmap for Safe Agent Adoption"}),"\n",(0,a.jsxs)(n.p,{children:["Effective AI agent deployment in banking requires coordinated ",(0,a.jsx)(n.strong,{children:"technical controls"}),", ",(0,a.jsx)(n.strong,{children:"governance"}),", and ",(0,a.jsx)(n.strong,{children:"operational safeguards"}),". This section outlines key design practices for balancing agency and autonomy, along with a phased rollout strategy."]}),"\n",(0,a.jsx)(n.h3,{id:"balancing-the-two-levers-design-principles",children:"Balancing the Two Levers: Design Principles"}),"\n",(0,a.jsx)(n.h4,{id:"1-limit-agency-for-high-stakes-tasks",children:"1. Limit agency for high-stakes tasks"}),"\n",(0,a.jsx)(n.p,{children:"Constrain agents handling sensitive decisions (e.g. credit or fund transfers) by hard-coding rules or using a \u201cpolicy wrapper\u201d to enforce strict decision boundaries."}),"\n",(0,a.jsx)(n.admonition,{title:"Example",type:"tip",children:(0,a.jsx)(n.p,{children:"A loan approval agent can only select from pre-approved loan products and must apply fixed eligibility rules\u2014no dynamic criteria changes or offer generation."})}),"\n",(0,a.jsx)(n.h4,{id:"2-calibrate-autonomy-to-maturity-and-risk",children:"2. Calibrate autonomy to maturity and risk"}),"\n",(0,a.jsx)(n.p,{children:"Start with low autonomy during early stages. Increase gradually with testing, and use conditional autonomy to trigger supervision when risks arise."}),"\n",(0,a.jsx)(n.admonition,{title:"Example",type:"tip",children:(0,a.jsx)(n.p,{children:"A customer support agent operates autonomously for FAQs but instantly drops to Level 2 autonomy when keywords like \u201cfraud\u201d or \u201ccomplaint\u201d are detected."})}),"\n",(0,a.jsx)(n.h4,{id:"3-add-human-guardrails-at-critical-points",children:"3. Add human guardrails at critical points"}),"\n",(0,a.jsx)(n.p,{children:"Insert human review or approval at key steps, using parallel checks or post-action audits with reversal capability."}),"\n",(0,a.jsx)(n.admonition,{title:"Example",type:"tip",children:(0,a.jsx)(n.p,{children:"Before issuing a provisional credit in a dispute, the agent prompts a back-office analyst for one-click approval, with justification auto-filled by the agent."})}),"\n",(0,a.jsx)(n.h4,{id:"4-compartmentalize-roles-with-multi-agent-design",children:"4. Compartmentalize roles with multi-agent design"}),"\n",(0,a.jsx)(n.p,{children:"Use specialized agents with limited scopes that collaborate, preventing any single agent from having unchecked end-to-end control."}),"\n",(0,a.jsx)(n.admonition,{title:"Example",type:"tip",children:(0,a.jsx)(n.p,{children:"A service agent drafts a message, a second agent verifies it against policy, and a third agent decides if human sign-off is required before sending."})}),"\n",(0,a.jsx)(n.h4,{id:"5-enable-manual-override-and-fallback-plans",children:"5. Enable manual override and fallback plans"}),"\n",(0,a.jsx)(n.p,{children:"Allow operators to halt agents instantly. Use fallback routes (e.g. human reps) and cautious rollouts like shadow mode or A/B testing."}),"\n",(0,a.jsx)(n.admonition,{title:"Example",type:"tip",children:(0,a.jsx)(n.p,{children:"All production agents are connected to a \u201ckill switch\u201d dashboard with human override rights and automated routing to live reps in case of errors or latency spikes."})}),"\n",(0,a.jsx)(n.h3,{id:"risk-analysis-and-failure-mode-mitigations",children:"Risk Analysis and Failure Mode Mitigations"}),"\n",(0,a.jsxs)(n.p,{children:["Performing a ",(0,a.jsx)(n.strong,{children:"Failure Modes and Effects Analysis (FMEA)"})," helps proactively identify weak points in AI agent design. Below are common failure modes and their corresponding mitigation strategies:"]}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:(0,a.jsx)(n.strong,{children:"Failure Mode"})}),(0,a.jsx)(n.th,{children:(0,a.jsx)(n.strong,{children:"Description"})}),(0,a.jsx)(n.th,{children:(0,a.jsx)(n.strong,{children:"Mitigation Strategy"})})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Hallucination / Misinformation"})}),(0,a.jsx)(n.td,{children:"LLM outputs inaccurate or misleading responses."}),(0,a.jsx)(n.td,{children:"Add verification steps, use retrieval-augmented generation (RAG), and clearly label AI-generated content."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Data Leakage / Privacy Breach"})}),(0,a.jsx)(n.td,{children:"AI exposes sensitive or personal data."}),(0,a.jsx)(n.td,{children:"Enforce strict data controls, mask/tokenize data, limit access scope, and sanitize inputs."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Unauthorized Action / Tool Misuse"})}),(0,a.jsx)(n.td,{children:"Agent takes unintended or unsafe actions."}),(0,a.jsx)(n.td,{children:"Whitelist approved tools, sandbox execution, set transaction limits, and adversarial-test workflows."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Coordination Failure (Multi-Agent)"})}),(0,a.jsx)(n.td,{children:"Agents miscommunicate or fail to synchronize tasks."}),(0,a.jsx)(n.td,{children:"Define clear orchestration logic, add timeouts, and log inter-agent interactions for auditing."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Model Drift / Performance Degradation"})}),(0,a.jsx)(n.td,{children:"Model quality decays or adapts undesirably over time."}),(0,a.jsx)(n.td,{children:"Set up continuous monitoring, champion\u2013challenger testing, and periodic model retraining or review."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Security Breaches / Adversarial Attacks"})}),(0,a.jsx)(n.td,{children:"AI system is manipulated or exploited by external inputs."}),(0,a.jsx)(n.td,{children:"Apply security hardening (e.g., input validation, isolation), rate-limit requests, and test adversarially."})]})]})]}),"\n",(0,a.jsxs)("p",{class:"center",children:[" ",(0,a.jsx)(n.em,{children:"Table 4: Failure Mode Mitigations"})," "]}),"\n",(0,a.jsx)(n.p,{children:"Thoughtful FMEA during agent design enables banks to embed safeguards upfront\u2014so systems fail safely, not silently."}),"\n",(0,a.jsx)(n.h3,{id:"illustrative-scenario-payment-dispute-resolution-workflow",children:"Illustrative Scenario: Payment Dispute Resolution Workflow"}),"\n",(0,a.jsx)(n.p,{children:"This multi-agent sequence highlights role separation and escalation controls:"}),"\n",(0,a.jsx)(n.mermaid,{value:"sequenceDiagram\n    participant Customer as Customer\n    participant FrontBot as Customer Chatbot Agent\n    participant ServiceAgent as Back-Office Service Agent\n    participant CompAgent as Compliance Agent\n    participant HumanOfficer as Human Compliance Officer\n\n    Customer->>FrontBot: Dispute $500 charge\n    FrontBot--\x3e>Customer: Acknowledges & requests details\n    FrontBot->>ServiceAgent: Open dispute case\n    ServiceAgent->>CompAgent: Check compliance\n    alt High-risk?\n        CompAgent--\x3e>HumanOfficer: Escalate for review\n        HumanOfficer--\x3e>CompAgent: Approve or act\n    end\n    CompAgent--\x3e>ServiceAgent: Compliance cleared\n    ServiceAgent->>ServiceAgent: Refund $500 provisionally\n    ServiceAgent--\x3e>FrontBot: Case opened, credit posted\n    FrontBot--\x3e>Customer: Confirms dispute & refund"}),"\n",(0,a.jsxs)("p",{class:"center",children:[" ",(0,a.jsx)(n.em,{children:"Figure 2: Example Payment Dispute Resolution Workflow"})," "]}),"\n",(0,a.jsx)(n.p,{children:"Each agent operates with scoped autonomy and clear escalation paths. All actions are auditable."}),"\n",(0,a.jsx)(n.h3,{id:"three-point-roadmap-pilot-scale-govern",children:"Three-Point Roadmap: Pilot, Scale, Govern"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Short Term \u2013 Pilot Phase:"})," Start with controlled pilots in sandbox or limited production settings. Choose valuable but not extreme-risk use cases. Keep autonomy low, require human review, and define KPIs/KRIs. Establish an AI governance committee to review pilot results."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Mid Term \u2013 Scaling and Integration:"})," After successful pilots, integrate agents with core systems and increase autonomy cautiously. Implement security, monitoring, and change-management controls, and engage regulators. Perform formal model validations or audits and define operational roles (e.g., \u201cAI controller\u201d)."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Long Term \u2013 Governance at Scale:"})," Institutionalize AI governance like other risk domains. Implement continuous monitoring dashboards, periodic retraining and change control, external assessments/certifications, and scenario planning for worst-case AI failures. Ensure board-level visibility and embed AI risk into routine audits."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"By following this phased roadmap, banks can iterate and learn in the early stages and avoid reckless \u201cbig bang\u201d deployments of unproven AI. Each phase builds the bank\u2019s AI maturity: from gaining foundational experience, to extending capabilities, to embedding robust governance that will serve for years to come."}),"\n",(0,a.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,a.jsxs)(n.p,{children:["Agentic AI offers banks powerful tools to streamline operations, personalize service, and enhance risk detection. But these benefits come with new risks. ",(0,a.jsx)(n.strong,{children:"Autonomy and agency must be carefully balanced"}),"\u2014excess in either can lead to compliance failures, customer harm, or systemic disruption."]}),"\n",(0,a.jsx)(n.p,{children:"Fortunately, banks aren\u2019t starting from scratch. Research and early industry lessons offer clear strategies: define autonomy levels, apply human oversight, and build in technical safeguards. This white paper outlined a practical path forward\u2014clarify agent roles, anticipate failures, and govern through layered controls and phased deployment."}),"\n",(0,a.jsx)(n.p,{children:"In short, safe AI adoption requires control, accountability, and thoughtful design. With the right guardrails, banks can unlock the full value of AI agents\u2014while earning the trust of regulators and customers alike."}),"\n",(0,a.jsx)(n.hr,{}),"\n","\n",(0,a.jsxs)(n.section,{"data-footnotes":!0,className:"footnotes",children:[(0,a.jsx)(n.h2,{className:"sr-only",id:"footnote-label",children:"Footnotes"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{id:"user-content-fn-a",children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsxs)(n.a,{href:"https://www.ciodive.com/news/ai-supervisor-role-growing-among-banks/805191/",children:["CIODIVE (2025). ",(0,a.jsx)(n.em,{children:"Banks turn to AI supervisors as agent use surges"})," (Nov. 12, 2025)."]})," ",(0,a.jsx)(n.a,{href:"#user-content-fnref-a","data-footnote-backref":"","aria-label":"Back to reference 1",className:"data-footnote-backref",children:"\u21a9"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{id:"user-content-fn-b",children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsxs)(n.a,{href:"https://www.consumerfinance.gov/data-research/research-reports/chatbots-in-consumer-finance/chatbots-in-consumer-finance/",children:["CFPB (2023). ",(0,a.jsx)(n.em,{children:"Chatbots in Consumer Finance"})," (Consumer Financial Protection Bureau report, June 2023)."]})," ",(0,a.jsx)(n.a,{href:"#user-content-fnref-b","data-footnote-backref":"","aria-label":"Back to reference 2",className:"data-footnote-backref",children:"\u21a9"})," ",(0,a.jsxs)(n.a,{href:"#user-content-fnref-b-2","data-footnote-backref":"","aria-label":"Back to reference 2-2",className:"data-footnote-backref",children:["\u21a9",(0,a.jsx)(n.sup,{children:"2"})]})," ",(0,a.jsxs)(n.a,{href:"#user-content-fnref-b-3","data-footnote-backref":"","aria-label":"Back to reference 2-3",className:"data-footnote-backref",children:["\u21a9",(0,a.jsx)(n.sup,{children:"3"})]})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{id:"user-content-fn-c",children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsxs)(n.a,{href:"https://www.cio.com/article/4089480/make-boards-responsible-for-ai-failures-banking-regulator-suggests.html",children:["Dunn, J. (2025). ",(0,a.jsx)(n.em,{children:"Make boards responsible for AI failures, banking regulator suggests"})," (CIO.com, Nov 13, 2025)."]})," ",(0,a.jsx)(n.a,{href:"#user-content-fnref-c","data-footnote-backref":"","aria-label":"Back to reference 3",className:"data-footnote-backref",children:"\u21a9"})," ",(0,a.jsxs)(n.a,{href:"#user-content-fnref-c-2","data-footnote-backref":"","aria-label":"Back to reference 3-2",className:"data-footnote-backref",children:["\u21a9",(0,a.jsx)(n.sup,{children:"2"})]})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{id:"user-content-fn-d",children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsxs)(n.a,{href:"https://arxiv.org/abs/2506.12469",children:["Feng et al. (2025). ",(0,a.jsx)(n.em,{children:"Levels of Autonomy for AI Agents"})," (arXiv preprint 2506.12469)."]})," ",(0,a.jsx)(n.a,{href:"#user-content-fnref-d","data-footnote-backref":"","aria-label":"Back to reference 4",className:"data-footnote-backref",children:"\u21a9"})," ",(0,a.jsxs)(n.a,{href:"#user-content-fnref-d-2","data-footnote-backref":"","aria-label":"Back to reference 4-2",className:"data-footnote-backref",children:["\u21a9",(0,a.jsx)(n.sup,{children:"2"})]})," ",(0,a.jsxs)(n.a,{href:"#user-content-fnref-d-3","data-footnote-backref":"","aria-label":"Back to reference 4-3",className:"data-footnote-backref",children:["\u21a9",(0,a.jsx)(n.sup,{children:"3"})]})," ",(0,a.jsxs)(n.a,{href:"#user-content-fnref-d-4","data-footnote-backref":"","aria-label":"Back to reference 4-4",className:"data-footnote-backref",children:["\u21a9",(0,a.jsx)(n.sup,{children:"4"})]})," ",(0,a.jsxs)(n.a,{href:"#user-content-fnref-d-5","data-footnote-backref":"","aria-label":"Back to reference 4-5",className:"data-footnote-backref",children:["\u21a9",(0,a.jsx)(n.sup,{children:"5"})]})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{id:"user-content-fn-e",children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsxs)(n.a,{href:"https://arxiv.org/abs/2509.22735",children:["Boddy & Joseph (2025). ",(0,a.jsx)(n.em,{children:"Regulating the Agency of LLM-based Agents"})," (arXiv preprint 2509.22735)."]})," ",(0,a.jsx)(n.a,{href:"#user-content-fnref-e","data-footnote-backref":"","aria-label":"Back to reference 5",className:"data-footnote-backref",children:"\u21a9"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{id:"user-content-fn-g",children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsxs)(n.a,{href:"https://www.bankofengland.co.uk/report/2024/artificial-intelligence-in-uk-financial-services-2024",children:["Bank of England & FCA (2024). ",(0,a.jsx)(n.em,{children:"Artificial Intelligence in UK Financial Services"})," (Survey Report)."]})," ",(0,a.jsx)(n.a,{href:"#user-content-fnref-g","data-footnote-backref":"","aria-label":"Back to reference 6",className:"data-footnote-backref",children:"\u21a9"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{id:"user-content-fn-f",children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsxs)(n.a,{href:"https://arxiv.org/abs/2502.05439",children:["Okpala et al. (2024). ",(0,a.jsx)(n.em,{children:"Agentic AI Systems Applied to Financial Services"})," (arXiv preprint 2502.05439)."]})," ",(0,a.jsx)(n.a,{href:"#user-content-fnref-f","data-footnote-backref":"","aria-label":"Back to reference 7",className:"data-footnote-backref",children:"\u21a9"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{id:"user-content-fn-h",children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsxs)(n.a,{href:"https://www.gartner.com/en/newsroom/press-releases/2025-06-25-gartner-predicts-over-40-percent-of-agentic-ai-projects-will-be-canceled-by-end-of-2027",children:["Gartner (2025). ",(0,a.jsx)(n.em,{children:"Press Release: Gartner Predicts Over 40% of Agentic AI Projects Will Be Canceled by 2027"})," (June 25, 2025)."]})," ",(0,a.jsx)(n.a,{href:"#user-content-fnref-h","data-footnote-backref":"","aria-label":"Back to reference 8",className:"data-footnote-backref",children:"\u21a9"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{id:"user-content-fn-i",children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsxs)(n.a,{href:"https://www.forbes.com/sites/brianbushard/2023/02/24/workers-chatgpt-use-restricted-at-more-banks-including-goldman-citigroup/",children:["Forbes (2023). ",(0,a.jsx)(n.em,{children:"Workers' ChatGPT Use Restricted at More Banks"})," (reporting on banks banning employee use of ChatGPT)."]})," ",(0,a.jsx)(n.a,{href:"#user-content-fnref-i","data-footnote-backref":"","aria-label":"Back to reference 9",className:"data-footnote-backref",children:"\u21a9"})]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},9813:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/balance-47bb6f8667017b1faa46dd0d12ee927b.png"}}]);