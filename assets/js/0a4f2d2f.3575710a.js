"use strict";(globalThis.webpackChunkai_whitepaper=globalThis.webpackChunkai_whitepaper||[]).push([[2310],{2315:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"selecting-the-right-ai-model","title":"Selecting the Right AI Model","description":"Executive Summary","source":"@site/docs/05-selecting-the-right-ai-model.md","sourceDirName":".","slug":"/selecting-the-right-ai-model","permalink":"/whitepaper/selecting-the-right-ai-model","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"banking","permalink":"/whitepaper/tags/banking"},{"inline":true,"label":"ai","permalink":"/whitepaper/tags/ai"},{"inline":true,"label":"agentic-ai","permalink":"/whitepaper/tags/agentic-ai"},{"inline":true,"label":"model","permalink":"/whitepaper/tags/model"},{"inline":true,"label":"llm","permalink":"/whitepaper/tags/llm"},{"inline":true,"label":"slm","permalink":"/whitepaper/tags/slm"},{"inline":true,"label":"nlm","permalink":"/whitepaper/tags/nlm"}],"version":"current","lastUpdatedAt":1763519191000,"sidebarPosition":5,"frontMatter":{"title":"Selecting the Right AI Model","date":"2025-11-13T10:00:00.000Z","slug":"selecting-the-right-ai-model","authors":["lkgarcia"],"tags":["banking","ai","agentic-ai","model","llm","slm","nlm"],"prompt":"Title: Selecting the Right AI Model: A Framework for Building Reliable and Scalable Agentic Systems\\n\\nMessage: Effective model selection in agentic AI for retail banking requires a disciplined framework that balances capability, adaptability, cost, latency, risk, and governance. This paper provides a decision model for choosing among Large Language Models (LLMs), Small Language Models (SLMs), Narrow / Specialized Models (NLMs), and adaptation strategies (prompt-only, retrieval augmentation, fine-tuning, multi-modal integration) to build reliable, scalable agent systems.\\n\\nPurpose: Deliver an actionable, evidence-based framework to guide architects and product leaders in selecting and adapting language and multi-modal models for high-impact retail banking use cases (e.g., personalized financial guidance, fraud signal triage, customer service orchestration) while controlling cost and risk.\\n\\nGoals:\\n  - Define classification taxonomy: LLM vs SLM vs NLM (capability, cost, latency, governance trade-offs).\\n  - Provide criteria for when to use: (a) LLM + context/prompt engineering only, (b) Retrieval-Augmented Generation (RAG), (c) Fine-tuning, (d) Multi-modal models.\\n  - Quantify cost & effort dimensions of fine-tuning (data acquisition, cleaning, annotation, infra, evaluation, ongoing drift management).\\n  - Outline evaluation dimensions: accuracy, hallucination rate, latency, cost per 1K tokens, privacy/compliance surface, operational maturity.\\n  - Present model selection decision flow (Mermaid diagram) from use case attributes (complexity, domain specificity, data readiness, regulatory sensitivity) to adaptation strategy.\\n  - Provide pragmatic 3-phase roadmap (short, mid, long-term) for model portfolio evolution in retail banking.\\n  - Highlight emerging role of multi-modal (text + document imaging, receipts, statements) and constraints.\\n  - Embed cost optimization levers (model sizing, caching, distillation, tiered routing, selective RAG scopes).\\n\\nAudience: Technical product managers, enterprise/solution architects, AI platform leads, and senior banking executives (mixed technical / non-technical). Assumes familiarity with AI concepts but not deep research expertise.\\n\\nTone: Clear, authoritative, pragmatic; moderately technical yet accessible; caveated where speculative; avoids hype and vendor promotion.\\n\\nLength & structure:\\n  - Target: ~1200\u20131600 words total.\\n  - Use plain Markdown headings (no bold), concise paragraphs, bullets, and one Mermaid diagram (decision flow).\\n  - Inline citations with footnotes: [^n] referencing reputable sources (ArXiv papers, Gartner insights, recognized universities/researchers).\\n  - Minimize jargon; define terms on first use.\\n\\nSections:\\n  1) Executive summary (100\u2013150 words): Core thesis + decision framework value.\\n  2) Introduction: Why disciplined model selection matters now (cost pressure, reliability, governance).\\n  3) Technical fundamentals:\\n     - Model scale vs capability vs latency (parameters, context window, throughput).\\n     - Adaptation continuum: prompt engineering \u2192 RAG \u2192 fine-tuning \u2192 multi-modal integration.\\n     - Evaluation & risk axes (accuracy, hallucination, compliance, cost kinetics, observability).\\n  4) Topics (deep dive):\\n     - Taxonomy: LLM vs SLM vs NLM (definition, strengths, limitations, when to choose each).\\n     - Strategy comparison: LLM + system prompt vs RAG vs Fine-Tuning (decision criteria: domain specificity, update frequency, data volume, risk tolerance).\\n     - Cost & effort model of fine-tuning (stages, typical resource consumption, breakeven thresholds).\\n     - Multi-modal models in retail banking (document ingestion, statement understanding, fraud evidence).\\n     - Cost dimension & optimization (pricing structures, batching, caching, distillation, model routing, guardrail execution costs).\\n     - Risk & governance (hallucination mitigation, leakage prevention, bias monitoring, auditability).\\n  5) Recommendations & roadmap:\\n     - Short-term: Establish evaluation harness, start with SLM or efficient LLM + robust system prompts + guardrails.\\n     - Mid-term: Introduce RAG over curated, governed internal knowledge base; selective fine-tuning for narrow classification tasks.\\n     - Long-term: Portfolio optimization (model routing & distillation), multi-modal expansion, continuous cost governance & drift management.\\n  6) Conclusion: Reinforce disciplined selection + adaptive strategy as foundation for sustainable agentic AI scaling.\\n\\nDecision diagram (Mermaid) guidance: Flow from use case inputs (domain specificity, update velocity, required explainability, available labeled data, latency budget, cost constraints) to recommended path (Prompt-only LLM, SLM, RAG, Fine-tune, Multi-modal extension).\\n\\nExamples & requirements:\\n  - 3-point roadmap (short / mid / long).\\n  - Emphasis on retail banking scenarios; avoid naming specific banks or jurisdictions.\\n  - No regulatory specifics; keep governance general (privacy, auditability).\\n\\nConstraints: Avoid vendor promotion; provide caveats for emerging capabilities; no unrealistic timelines; no unsupported performance claims.\\n\\nSources (target at least 6 distinct footnotes): Mix of ArXiv (model scaling, RAG efficiency), Gartner (enterprise adoption / cost trends), and leading academic institutions (e.g., Stanford, MIT, Oxford) on evaluation, hallucination mitigation, and multi-modal advances.\\n\\nOutput Format: Single downloadable Markdown file ready for Docusaurus docs pipeline.\\n"},"sidebar":"docsSidebar","previous":{"title":"Enabling Agentic AI Through Well-Defined API Contracts","permalink":"/whitepaper/enabling-agentic-ai-through-well-defined-api-contracts"},"next":{"title":"High-Impact Use Cases and Strategic Insights","permalink":"/whitepaper/high-impact-use-cases"}}');var a=i(4848),s=i(8453);const o={title:"Selecting the Right AI Model",date:new Date("2025-11-13T10:00:00.000Z"),slug:"selecting-the-right-ai-model",authors:["lkgarcia"],tags:["banking","ai","agentic-ai","model","llm","slm","nlm"],prompt:"Title: Selecting the Right AI Model: A Framework for Building Reliable and Scalable Agentic Systems\n\nMessage: Effective model selection in agentic AI for retail banking requires a disciplined framework that balances capability, adaptability, cost, latency, risk, and governance. This paper provides a decision model for choosing among Large Language Models (LLMs), Small Language Models (SLMs), Narrow / Specialized Models (NLMs), and adaptation strategies (prompt-only, retrieval augmentation, fine-tuning, multi-modal integration) to build reliable, scalable agent systems.\n\nPurpose: Deliver an actionable, evidence-based framework to guide architects and product leaders in selecting and adapting language and multi-modal models for high-impact retail banking use cases (e.g., personalized financial guidance, fraud signal triage, customer service orchestration) while controlling cost and risk.\n\nGoals:\n  - Define classification taxonomy: LLM vs SLM vs NLM (capability, cost, latency, governance trade-offs).\n  - Provide criteria for when to use: (a) LLM + context/prompt engineering only, (b) Retrieval-Augmented Generation (RAG), (c) Fine-tuning, (d) Multi-modal models.\n  - Quantify cost & effort dimensions of fine-tuning (data acquisition, cleaning, annotation, infra, evaluation, ongoing drift management).\n  - Outline evaluation dimensions: accuracy, hallucination rate, latency, cost per 1K tokens, privacy/compliance surface, operational maturity.\n  - Present model selection decision flow (Mermaid diagram) from use case attributes (complexity, domain specificity, data readiness, regulatory sensitivity) to adaptation strategy.\n  - Provide pragmatic 3-phase roadmap (short, mid, long-term) for model portfolio evolution in retail banking.\n  - Highlight emerging role of multi-modal (text + document imaging, receipts, statements) and constraints.\n  - Embed cost optimization levers (model sizing, caching, distillation, tiered routing, selective RAG scopes).\n\nAudience: Technical product managers, enterprise/solution architects, AI platform leads, and senior banking executives (mixed technical / non-technical). Assumes familiarity with AI concepts but not deep research expertise.\n\nTone: Clear, authoritative, pragmatic; moderately technical yet accessible; caveated where speculative; avoids hype and vendor promotion.\n\nLength & structure:\n  - Target: ~1200\u20131600 words total.\n  - Use plain Markdown headings (no bold), concise paragraphs, bullets, and one Mermaid diagram (decision flow).\n  - Inline citations with footnotes: [^n] referencing reputable sources (ArXiv papers, Gartner insights, recognized universities/researchers).\n  - Minimize jargon; define terms on first use.\n\nSections:\n  1) Executive summary (100\u2013150 words): Core thesis + decision framework value.\n  2) Introduction: Why disciplined model selection matters now (cost pressure, reliability, governance).\n  3) Technical fundamentals:\n     - Model scale vs capability vs latency (parameters, context window, throughput).\n     - Adaptation continuum: prompt engineering \u2192 RAG \u2192 fine-tuning \u2192 multi-modal integration.\n     - Evaluation & risk axes (accuracy, hallucination, compliance, cost kinetics, observability).\n  4) Topics (deep dive):\n     - Taxonomy: LLM vs SLM vs NLM (definition, strengths, limitations, when to choose each).\n     - Strategy comparison: LLM + system prompt vs RAG vs Fine-Tuning (decision criteria: domain specificity, update frequency, data volume, risk tolerance).\n     - Cost & effort model of fine-tuning (stages, typical resource consumption, breakeven thresholds).\n     - Multi-modal models in retail banking (document ingestion, statement understanding, fraud evidence).\n     - Cost dimension & optimization (pricing structures, batching, caching, distillation, model routing, guardrail execution costs).\n     - Risk & governance (hallucination mitigation, leakage prevention, bias monitoring, auditability).\n  5) Recommendations & roadmap:\n     - Short-term: Establish evaluation harness, start with SLM or efficient LLM + robust system prompts + guardrails.\n     - Mid-term: Introduce RAG over curated, governed internal knowledge base; selective fine-tuning for narrow classification tasks.\n     - Long-term: Portfolio optimization (model routing & distillation), multi-modal expansion, continuous cost governance & drift management.\n  6) Conclusion: Reinforce disciplined selection + adaptive strategy as foundation for sustainable agentic AI scaling.\n\nDecision diagram (Mermaid) guidance: Flow from use case inputs (domain specificity, update velocity, required explainability, available labeled data, latency budget, cost constraints) to recommended path (Prompt-only LLM, SLM, RAG, Fine-tune, Multi-modal extension).\n\nExamples & requirements:\n  - 3-point roadmap (short / mid / long).\n  - Emphasis on retail banking scenarios; avoid naming specific banks or jurisdictions.\n  - No regulatory specifics; keep governance general (privacy, auditability).\n\nConstraints: Avoid vendor promotion; provide caveats for emerging capabilities; no unrealistic timelines; no unsupported performance claims.\n\nSources (target at least 6 distinct footnotes): Mix of ArXiv (model scaling, RAG efficiency), Gartner (enterprise adoption / cost trends), and leading academic institutions (e.g., Stanford, MIT, Oxford) on evaluation, hallucination mitigation, and multi-modal advances.\n\nOutput Format: Single downloadable Markdown file ready for Docusaurus docs pipeline.\n"},r="Selecting the Right AI Model: A Framework for Reliable and Scalable Agentic Systems",l={},c=[{value:"Executive Summary",id:"executive-summary",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Technical Fundamentals",id:"technical-fundamentals",level:2},{value:"Deep Dive Topics",id:"deep-dive-topics",level:2},{value:"Taxonomy of Model Options: LLM vs SLM vs NLM",id:"taxonomy-of-model-options-llm-vs-slm-vs-nlm",level:3},{value:"Choosing an Adaptation Strategy: Prompting vs RAG vs Fine-Tuning",id:"choosing-an-adaptation-strategy-prompting-vs-rag-vs-fine-tuning",level:3},{value:"Cost and Effort of Fine-Tuning",id:"cost-and-effort-of-fine-tuning",level:3},{value:"Multi-Modal Models in Retail Banking",id:"multi-modal-models-in-retail-banking",level:3},{value:"Cost Optimization Techniques",id:"cost-optimization-techniques",level:3},{value:"Risk and Governance Considerations",id:"risk-and-governance-considerations",level:3},{value:"Recommendations &amp; Roadmap",id:"recommendations--roadmap",level:2},{value:"Conclusion",id:"conclusion",level:2},{value:"Footnotes",id:"footnotes",level:2}];function d(e){const n={a:"a",admonition:"admonition",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.admonition,{title:"WORK IN PROGRESS",type:"warning"}),"\n",(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"selecting-the-right-ai-model-a-framework-for-reliable-and-scalable-agentic-systems",children:"Selecting the Right AI Model: A Framework for Reliable and Scalable Agentic Systems"})}),"\n",(0,a.jsx)(n.h2,{id:"executive-summary",children:"Executive Summary"}),"\n",(0,a.jsxs)(n.p,{children:["Agentic AI systems in retail banking promise personalized guidance, smarter fraud detection, and improved customer service. Yet choosing the ",(0,a.jsx)(n.em,{children:"right"})," AI models and adaptation strategies is critical to balance capability with cost, speed, and risk. This paper presents a structured decision framework for model selection. We define a taxonomy of model scales \u2013 from Large Language Models (LLMs) to Small Language Models (SLMs) and narrow domain models \u2013 and outline criteria for when to use each. We compare prompt engineering versus Retrieval-Augmented Generation (RAG) versus fine-tuning, and discuss emerging multi-modal AI (combining text with images or other inputs). We quantify the often-underestimated costs of fine-tuning (data collection, annotation, infrastructure) and highlight evaluation dimensions like accuracy, latency, hallucination rates, and compliance. Finally, we provide a phased roadmap for adopting and evolving an optimal model portfolio. By following a disciplined selection approach, banking innovators can deploy AI agents that are ",(0,a.jsx)(n.strong,{children:"reliable, scalable, and cost-effective"})," \u2013 avoiding the pitfalls that cause many AI projects to stall1."]}),"\n",(0,a.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsxs)(n.p,{children:["As generative AI moves past the initial hype, banks face pressure to deliver value while controlling costs and risks. Recent surveys predict ",(0,a.jsx)(n.strong,{children:"at least 30% of enterprise GenAI projects will be abandoned by 2025"})," due to poor data quality, inadequate risk controls, escalating costs or unclear ROI1. In retail banking \u2013 where accuracy and compliance are paramount \u2013 blindly deploying the largest model is no longer tenable. ",(0,a.jsx)(n.strong,{children:"Cost has become as big an AI risk as security or hallucinations"}),", according to Gartner2. Meanwhile, regulators and customers demand reliable, explainable outcomes. In this context, ",(0,a.jsx)(n.em,{children:"model selection"})," is not just a technical decision but a strategic one. Choosing an appropriate model (or combination) can mean the difference between an AI assistant that safely streamlines operations versus one that overruns budget or misleads users. This paper argues that a disciplined framework for model selection and adaptation is now essential. By systematically aligning use cases with the right model scale and approach, banks can achieve strong AI capabilities ",(0,a.jsx)(n.strong,{children:"while maintaining control over latency, cost, and governance"}),"."]}),"\n",(0,a.jsx)(n.h2,{id:"technical-fundamentals",children:"Technical Fundamentals"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Model Scale vs. Capability vs. Latency:"})," Modern language models range from billions to hundreds of billions of parameters. Generally, ",(0,a.jsx)(n.strong,{children:"larger models demonstrate broader knowledge and more sophisticated reasoning, but with higher cost and slower responses"}),". For example, a 70B-parameter model might answer complex queries more accurately than a 7B model, but will incur significantly more latency and expense per query. In one benchmark, moving from a 1B to a 70B model increased inference latency roughly 10\xd7 (0.2s to 3s for the same output length) and ",(0,a.jsx)(n.em,{children:"per-token costs by over 7\xd7"})," under comparable conditions",(0,a.jsx)(n.a,{href:"https://openreview.net/pdf?id=pZFJLsIY2m#:~:text=restricts%20real,1",children:"openreview.net"}),". In other words, scale brings diminishing returns: accuracy does not increase linearly with size3. Smaller models (e.g. 1\u201315B parameters) can often match larger models on narrow or structured tasks when fine-tuned, and they can deliver responses in tens of milliseconds instead of seconds3. The trade-off is clear \u2013 ",(0,a.jsx)(n.strong,{children:"bigger isn\u2019t always better"}),"; the optimal size depends on task complexity and real-time requirements. Key performance metrics to monitor include time-to-first-token (how quickly the model starts responding), throughput (requests per second it can handle), and memory/compute footprint."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Adaptation Continuum:"})," Beyond choosing a base model, architects must decide how to ",(0,a.jsx)(n.em,{children:"adapt"})," it to the task. Approaches fall along a continuum of effort and flexibility:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Prompt Engineering (Context-Only):"})," Using carefully crafted prompts, system instructions, and examples to guide a general model\u2019s behavior without changing its weights. This is the fastest to implement \u2013 requiring no model training \u2013 and works well when a powerful LLM can solve the task with general knowledge and a bit of guidance. However, prompt-only solutions may struggle with highly domain-specific queries or factual accuracy on enterprise data."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Retrieval-Augmented Generation (RAG):"})," Here the model remains frozen, but is supplied with relevant ",(0,a.jsx)(n.em,{children:"external context"})," at query time (typically via a vector database lookup). RAG allows systems to ",(0,a.jsx)(n.strong,{children:"inject up-to-date, domain-specific information into the model\u2019s context"}),", mitigating the model\u2019s knowledge cutoff or limited training data. This can greatly improve factual accuracy and reduce hallucinations4. The trade-off is added complexity: one must maintain a knowledge repository and ensure the retriever finds high-quality context. RAG introduces latency from the retrieval step, but pipelines can be optimized to still meet interactive speeds. Crucially, RAG ",(0,a.jsx)(n.strong,{children:"improves transparency and verifiability"}),", since the model\u2019s answers are grounded in retrieved evidence",(0,a.jsx)(n.a,{href:"https://www.mdpi.com/2504-4990/6/4/116#:~:text=Our%20approach%20leverages%20the%20inherent,possible%2C%20especially%20in%20scenarios%20where",children:"mdpi.com"}),(0,a.jsx)(n.a,{href:"https://www.mdpi.com/2504-4990/6/4/116#:~:text=technique%20used%20for%20enhancing%20the,then%20encoded%20into%20a%20vectorised",children:"mdpi.com"}),"."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Fine-Tuning (Domain Adaptation):"})," Fine-tuning entails training the model on domain-specific examples or instructions so that it ",(0,a.jsx)(n.em,{children:"internalizes"})," task knowledge. This can yield strong performance on specialized tasks and reduce the need for elaborate prompts at runtime. Techniques range from full model fine-tuning to parameter-efficient methods (e.g. low-rank adapters or LoRA). Fine-tuning an LLM, however, is a non-trivial project \u2013 it requires assembling quality training data, running experiments on GPUs, and evaluating carefully. ",(0,a.jsx)(n.strong,{children:"The costs can be significant"}),", both in one-time training spend and ongoing maintenance as data or requirements change. We discuss these costs in depth later. Fine-tuning is best reserved for scenarios where high accuracy on a well-defined task justifies the investment, or where data privacy requires an on-prem model that \u201cknows\u201d internal data without retrieval."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Multi-Modal Integration:"})," Many banking use cases involve not just text but documents, forms, images of IDs or checks, audio from calls, etc. Multi-modal models extend language models with vision, speech, or structured data capabilities. For example, a multi-modal AI assistant might analyze a photo of a handwritten check along with a customer\u2019s query. Such models are emerging rapidly \u2013 Gartner projects **40% of GenAI solutions will be multimodal by 2027 (up from just 1% in 2023)**5. Adopting multi-modality can unlock richer functionality (e.g. reading financial statements or parsing transaction receipts), but also raises complexity in model selection and deployment. Often it involves combining separate specialized models (which can introduce latency and integration challenges) or using a very large model natively trained on mixed data. We must consider whether multi-modal capability is essential for a given use case, or if simpler pipelines (like extracting text from documents then using an LLM) suffice in the near term."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Evaluation and Risk Axes:"})," When evaluating model options, it is critical to go beyond raw accuracy on paper. A holistic evaluation should encompass multiple axes6:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Accuracy & Relevance:"})," Does the model output correct and useful answers for the task? This includes factual accuracy for knowledge queries and numerical accuracy for calculations."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Hallucination Rate:"})," How often does the model \u201cmake up\u201d an answer? Hallucination is especially concerning in finance (e.g. fabricating a regulatory requirement or a transaction that never occurred). Techniques like RAG and prompt constraints help here, but measurement is key \u2013 e.g. track the percentage of responses containing unverifiable claims."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Latency:"})," Response time under expected load. An otherwise accurate model that takes 10 seconds per query may fail in a live chat context. Set targets (e.g. < 2 seconds for customer-facing agents) and test with realistic concurrent usage."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Cost per Query:"})," Roughly, the compute or API expense per 1000 tokens for each model. This can vary by an order of magnitude between model choices (as shown in the earlier example of 1B vs 70B token pricing",(0,a.jsx)(n.a,{href:"https://openreview.net/pdf?id=pZFJLsIY2m#:~:text=restricts%20real,1",children:"openreview.net"}),"). Estimating cost at scale prevents sticker shock once deployed."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Privacy and Compliance:"})," Does using the model introduce data residency or privacy concerns? Smaller on-prem models give more control, whereas third-party LLM APIs might send sensitive data off-site (needing encryption or redaction). Also, assess if the model has mechanisms for auditing its outputs or explaining decisions \u2013 important for compliance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Robustness and Bias:"})," How does the model perform on edge cases or biased inputs? Are there guardrails to prevent toxic or biased outputs? Evaluation should include stress-testing for unacceptable outputs, especially given fairness and ethical expectations in banking."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Operational Maturity:"})," Consider the tooling and community support for the model. Mature models have better monitoring, debuggability, and fine-tuning support. An open-source model might allow internal audit of its weights, whereas a closed API might offer built-in monitoring tools. Ensure you can log interactions and retrain or update the model over time (for example, to handle model drift as language or products evolve)."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"By scoring options across these dimensions, decision-makers can surface trade-offs clearly \u2013 for instance, Model A might score highest on accuracy but pose greater compliance overhead, whereas Model B is slightly less accurate but far cheaper and faster."}),"\n",(0,a.jsx)(n.h2,{id:"deep-dive-topics",children:"Deep Dive Topics"}),"\n",(0,a.jsx)(n.h3,{id:"taxonomy-of-model-options-llm-vs-slm-vs-nlm",children:"Taxonomy of Model Options: LLM vs SLM vs NLM"}),"\n",(0,a.jsx)(n.p,{children:"Not all \u201cAI models\u201d are created equal. It\u2019s useful to classify the types of models available:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Large Language Models (LLMs):"})," These are the big generalists \u2013 typically with ",(0,a.jsx)(n.strong,{children:"tens or hundreds of billions of parameters, trained on massive text corpora"}),". Examples include GPT-4-class models or PaLM. LLMs excel at open-ended understanding and reasoning. They can follow complex instructions and have extensive world knowledge. For a bank, an LLM could answer a wide range of customer questions or draft detailed reports with minimal task-specific training. However, they come with ",(0,a.jsx)(n.strong,{children:"significant drawbacks in enterprise settings"}),": high inference costs, increased latency, and often they run only via cloud APIs (raising data governance concerns). Fine-tuning an LLM on proprietary data is also expensive (often requiring specialist hardware and weeks of effort). As one recent survey noted, LLMs face ",(0,a.jsx)(n.em,{children:"\u201chigh fine-tuning costs, inference latency, limited edge deployability, and reliability concerns\u201d"})," despite their impressive capabilities3. Thus, LLMs are powerful but heavy tools \u2013 best reserved for when the use case truly demands top-tier language prowess across broad knowledge."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Small/Medium Language Models (SLMs):"})," There is no hard cutoff, but SLMs typically range from ~100 million up to 6\u201315 billion parameters. These models are ",(0,a.jsx)(n.strong,{children:"orders of magnitude more efficient"}),", often able to run on a single GPU or even CPU with sub-second latency3. Examples include distilled versions of larger models or bespoke smaller architectures. While an SLM may not match an LLM on complex reasoning out-of-the-box, it can perform remarkably well on domain-specific tasks ",(0,a.jsx)(n.strong,{children:"when fine-tuned or provided relevant context"}),". In fact, SLMs fine-tuned on a focused dataset can rival or outperform far larger models on that niche (for example, a 7B model fine-tuned on banking FAQs might beat a 70B model that has never seen those). Enterprise teams favor SLMs for ",(0,a.jsx)(n.strong,{children:"greater control and cost savings"}),": these models can often be deployed on-premises (addressing data privacy), updated more frequently, and scaled to millions of queries economically. One industry analysis estimates that training cutting-edge LLMs costs over $100M, and even inference pricing from vendors grows steeply with model size, whereas using SLMs can ",(0,a.jsx)(n.em,{children:"cut cost-per-query by two orders of magnitude"})," in production3. In short, SLMs trade some generality for ",(0,a.jsx)(n.strong,{children:"speed, affordability, and ease of governance"}),", which is often a smart trade-off for well-understood banking applications."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Narrow or Specialized Models (NLMs):"})," We use this term to denote models that are ",(0,a.jsx)(n.strong,{children:"purpose-built for a specific domain or task"}),". A narrow model could be an LLM or an SLM \u2013 size is secondary to specialization. For example, a 20B-parameter model trained exclusively on financial texts, or a refined version of a general model tuned for credit risk assessment, would qualify as a narrow model. These often originate from fine-tuning an existing model on domain data (e.g. tuning an open-source base on a bank\u2019s documents) or from training a model from scratch on targeted data. The strength of NLMs is ",(0,a.jsx)(n.strong,{children:"precision and relevance"}),": by focusing on a limited scope, they can achieve high accuracy and use terminology correctly (reducing irrelevant or incorrect outputs for that domain). NLMs also tend to be more efficient for their domain, since they aren\u2019t burdened by knowledge of unrelated topics. The downside is brittleness outside their specialty \u2013 a narrow model might fail if asked something slightly out-of-distribution. In practice, many enterprise AI solutions compose multiple narrow models, each tackling a piece of the problem (for example, one model classifies transaction anomalies while another generates customer responses). When we refer to NLMs, think ",(0,a.jsx)(n.em,{children:"\u201csmall or large, but specifically fine-tuned for our needs.\u201d"})," These require effort to build and maintain but can deliver superior results for high-value tasks like fraud detection, where general models may not be as reliable on the nuances."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["It is worth noting that these categories overlap. A given solution might use an LLM for general reasoning but an SLM for a particular tool-like function. Also, ",(0,a.jsx)(n.strong,{children:"open-source vs. closed-source"})," is another consideration orthogonal to size: open models (often SLMs) allow more customization and on-site deployment, whereas API models (often LLMs) might offer cutting-edge performance but with vendor dependency. Organizations should inventory their use cases and identify which category (or combination) fits each scenario \u2013 rather than defaulting to one model for all."]}),"\n",(0,a.jsx)(n.h3,{id:"choosing-an-adaptation-strategy-prompting-vs-rag-vs-fine-tuning",children:"Choosing an Adaptation Strategy: Prompting vs RAG vs Fine-Tuning"}),"\n",(0,a.jsx)(n.p,{children:"Selecting the base model is step one; next comes deciding how that model will incorporate domain knowledge and stay current. We compare three prevalent strategies:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"LLM + System Prompt Only:"})," ",(0,a.jsx)(n.em,{children:"When to use:"})," If your use case is relatively general (e.g. conversational FAQs, common-sense reasoning) and doesn\u2019t require up-to-the-minute data, you might simply use a strong LLM out-of-the-box. With thoughtful system prompts and maybe a few examples, the model can often handle tasks via zero-shot or few-shot learning. This approach shines for prototypes or when you need to support a wide variety of queries without building a custom pipeline. It minimizes initial effort \u2013 no model training needed \u2013 but ",(0,a.jsx)(n.strong,{children:"watch out for hallucinations or knowledge gaps"}),". For instance, asking a raw LLM about your bank\u2019s latest policies could yield confident ",(0,a.jsx)(n.em,{children:"but incorrect"})," answers. This strategy also struggles if the model must reliably cite sources or process customer-specific data (like account info). Use prompt-only when the convenience outweighs the risk, and implement guardrails (like pattern-based checks or fallbacks) to handle when the model\u2019s response might be off the mark."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Retrieval-Augmented Generation (RAG):"})," ",(0,a.jsx)(n.em,{children:"When to use:"})," If the task requires ",(0,a.jsx)(n.strong,{children:"incorporating proprietary or dynamic information"})," (e.g. a virtual assistant giving personalized financial advice using a customer\u2019s transaction history, or an AI tool that answers questions about internal banking procedures), RAG is a compelling approach. Rather than trying to encode all that knowledge in the model\u2019s weights via fine-tuning, RAG keeps a ",(0,a.jsx)(n.strong,{children:"searchable knowledge base"})," (documents, database entries, etc.). At query time, the system retrieves relevant snippets and prepends them to the model\u2019s prompt. The LLM or SLM then generates an answer ",(0,a.jsx)(n.em,{children:"grounded in those snippets"}),". The benefit is two-fold: your AI\u2019s knowledge is easily updatable (just add or edit documents in the knowledge store) and ",(0,a.jsx)(n.strong,{children:"factual accuracy improves"}),", since the model has the sources in front of it. Studies show retrieval can significantly cut down hallucination rates \u2013 one analysis found integrating retrieval reduced incorrect statements by 42\u201368% in tested scenarios4. Moreover, users and auditors can often trace the answer back to a source document, aiding explainability. The downsides include the engineering overhead of maintaining the retrieval system and ensuring data coverage. Also, if the retrieval fails (no relevant document found), the model might still stumble. RAG is ideal when ",(0,a.jsx)(n.strong,{children:"your domain data is extensive and changes frequently"})," (e.g. compliance rules, product details) or when answers must cite evidence. Many high-impact banking uses (fraud investigation assistants, research tools for analysts, customer support bots connected to knowledge bases) will benefit from a RAG architecture."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Fine-Tuning the Model:"})," ",(0,a.jsx)(n.em,{children:"When to use:"})," If the use case is ",(0,a.jsx)(n.em,{children:"narrowly defined, high-volume, and critical for accuracy"}),", and you have quality training data (or can create it), fine-tuning can pay off. Fine-tuning means your model effectively ",(0,a.jsx)(n.em,{children:"learns"})," the task \u2013 for example, classifying loan applications as approved/denied based on historical data, or generating tailored marketing messages in the brand tone. This often yields the ",(0,a.jsx)(n.strong,{children:"best task performance"})," because the model parameters are optimized for your specific data. Fine-tuning is also the go-to when the model needs to handle ",(0,a.jsx)(n.em,{children:"nuanced outputs or formats"})," (like generating SQL queries or regulatory reports) that are hard to reliably get via prompting alone. However, the commitment is substantial: one must gather and label data, invest in training runs, evaluate on validation sets, and possibly repeat this process to adjust to drift (e.g. new regulatory changes requiring model update). ",(0,a.jsx)(n.strong,{children:"Data availability is usually the deciding factor"})," \u2013 if you don\u2019t have a large, representative dataset for the task, fine-tuning a big model is ill-advised. In such cases, prompt-based or retrieval approaches are safer. Additionally, fine-tuning large models can be very costly and slow (even with techniques like parameter-efficient tuning). An emerging best practice is to fine-tune smaller open-source models for specific tasks, while using a large model for general reasoning or as a fallback. This \u201chybrid\u201d approach can give the best of both: the fine-tuned NLM for what it\u2019s great at, and an LLM for everything else. We\u2019ll discuss cost considerations next \u2013 in short, ",(0,a.jsx)(n.strong,{children:"fine-tuning is an investment"})," that should be justified by a clear business case (e.g. expected improvement in accuracy or user experience that outweighs the effort)."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["Finally, note that these strategies aren\u2019t mutually exclusive. For instance, you might fine-tune a model and ",(0,a.jsx)(n.em,{children:"also"})," use retrieval augmentation with it to inject new facts. Or use prompting and RAG together with an LLM. The art of solution design is picking the minimal complexity approach that meets the requirements. A decision flow diagram can help choose a path (see ",(0,a.jsx)(n.strong,{children:"Figure 1"}),")."]}),"\n",(0,a.jsx)(n.mermaid,{value:'flowchart TD\n    A([Use Case Attributes]) --\x3e B{High domain specificity?}\n    B -- "Yes" --\x3e C{Labeled domain data available?}\n    C -- "Yes" --\x3e C1["Fine-Tune a Specialized Model (NLM or SLM)"]\n    C -- "No" --\x3e C2[RAG: LLM with Retrieval Augmentation]\n    B -- "No" --\x3e D{High reasoning complexity?}\n    D -- "Yes" --\x3e D1["Use Large Language Model (LLM) with prompts"]\n    D -- "No" --\x3e E{Strict latency or cost constraints?}\n    E -- "Yes" --\x3e E1["Use Small/Efficient Model (SLM), possibly fine-tuned"]\n    E -- "No" --\x3e E2["Use LLM (general model)"]\n    C1 --\x3e M{"Multi-modal inputs (text + images)?"}\n    C2 --\x3e M\n    D1 --\x3e M\n    E1 --\x3e M\n    E2 --\x3e M\n    M -- "Yes" --\x3e M1[Incorporate Multi-Modal Model or Vision Module]\n    M -- "No" --\x3e X([Deploy Solution])'}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.em,{children:"Figure 1: Decision flow for model selection."})," From top, assess if the problem is domain-specific and whether you have data to fine-tune a custom model. If not, use retrieval to inject domain knowledge into a general model. For general-use cases, decide based on task complexity: simple tasks with tight speed/cost budgets favor smaller models, whereas complex reasoning might require a large model. In all cases, if the use case involves non-text data (e.g. document images), consider a multi-modal extension."]}),"\n",(0,a.jsx)(n.h3,{id:"cost-and-effort-of-fine-tuning",children:"Cost and Effort of Fine-Tuning"}),"\n",(0,a.jsxs)(n.p,{children:["Fine-tuning deserves a closer look because its costs are often ",(0,a.jsx)(n.em,{children:"underestimated"}),". It\u2019s tempting to assume we can just train the model on our data and achieve a magic boost in accuracy. In reality, successful fine-tuning in an enterprise setting entails:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Data Collection and Labeling:"})," Obtaining a high-quality dataset of task examples is usually the hardest part. In retail banking, this might mean months of historical chat transcripts annotated with correct responses, or thousands of transaction records labeled as fraudulent or not. The labor (often involving domain experts) to create these labels can be very expensive \u2013 recent analyses indicate that the ",(0,a.jsx)(n.strong,{children:"human annotation cost now often exceeds the computational cost of training"})," large models7. For example, one study found that for state-of-the-art models in 2024, the total spend on human-provided fine-tuning data was about ",(0,a.jsx)(n.em,{children:"3\xd7 the spend on compute hardware"})," for training those models7. This trend is only growing as model training becomes more efficient but quality data remains scarce. In short, be prepared to invest in data as a first-class cost."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Compute Infrastructure:"})," Fine-tuning large models requires robust infrastructure (GPUs or TPUs) and MLOps pipelines. Even with cloud offerings, the ",(0,a.jsx)(n.strong,{children:"training fees can rack up quickly"}),". As a ballpark, fine-tuning a 10B+ parameter model on a moderately sized dataset could cost tens of thousands of dollars in cloud compute, not including iteration if you need to experiment with hyperparameters. Smaller models or parameter-efficient methods (like using LoRA adapters or low-bit quantization) can significantly cut this cost, and are recommended unless full model tuning is absolutely necessary. Also consider ",(0,a.jsx)(n.em,{children:"opportunity cost"}),": engineering time to set up and manage these jobs."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Evaluation and Validation:"})," You cannot just fine-tune and deploy blindly, especially in a regulated domain. You need held-out test data and possibly a human evaluation process to ensure the tuned model performs as expected and hasn\u2019t overfit or picked up new biases. This step might involve additional annotation (for test sets or for evaluating outputs for things like bias or compliance issues). Plan time for a rigorous evaluation cycle, including adversarial testing (e.g. does a fine-tuned chatbot hallucinate less, or perhaps more confidently now? Has it forgotten how to handle inputs outside the fine-tuned domain?)."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Deployment and Monitoring:"})," Once a fine-tuned model is in production, monitoring is crucial to catch drift or performance regressions. If the model starts making errors as data trends shift (say, new types of fraud emerge), you need a process to collect new data and re-train or fine-tune again. This ongoing maintenance can be significant. Think of a fine-tuned model as a new ",(0,a.jsx)(n.em,{children:"software asset"})," that requires lifecycle management: versioning, change audits, periodic retraining, etc., especially as ",(0,a.jsx)(n.strong,{children:"regulations or product definitions change over time"}),"."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Risk Mitigation:"})," Fine-tuning can inadvertently introduce or amplify biases present in the fine-tuning data, or cause the model to become too confident in a narrow area. It may also leak proprietary data if not done carefully (for instance, if internal data is used to fine-tune a model that is then publicly accessible). Governance steps like data anonymization, bias checks, and security reviews of the model are advisable. Unlike prompting or retrieval, where the base model remains unchanged (and often well-tested), a fine-tuned model is unique \u2013 its failures are your responsibility. This is manageable with proper processes but is a heavier lift in terms of accountability."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["When do these costs and efforts become \u201cworth it\u201d? One rule of thumb is to fine-tune only if the expected scale of use and required accuracy of the task will provide a strong return on that investment. For example, if automating a certain decision via a fine-tuned model could save millions of dollars or significantly reduce manual work, and no off-the-shelf model can do it, the investment is justified. Another case is if fine-tuning a smaller open model can drastically reduce reliance on an expensive API model \u2013 the breakeven might come after a certain number of queries served. ",(0,a.jsx)(n.strong,{children:"Banks should quantify these trade-offs"}),". In some cases, starting with a prompt/RAG approach to validate impact, then upgrading to a fine-tuned model for efficiency at scale, can be a pragmatic path."]}),"\n",(0,a.jsxs)(n.p,{children:["It\u2019s also worth noting new techniques that reduce fine-tuning effort: ",(0,a.jsx)(n.em,{children:"few-shot fine-tuning"})," (using relatively few examples combined with clever prompting), ",(0,a.jsx)(n.em,{children:"reward model tuning"})," (RLHF) to align outputs, and ",(0,a.jsx)(n.em,{children:"modular training"})," (like adding a small domain expert model that works alongside the general model). These can lower data or compute requirements. But whichever approach, always include the costs of data, people, and long-term maintenance in your project planning \u2013 not just the one-time training bill."]}),"\n",(0,a.jsx)(n.h3,{id:"multi-modal-models-in-retail-banking",children:"Multi-Modal Models in Retail Banking"}),"\n",(0,a.jsx)(n.p,{children:"Many transformative banking AI use cases go beyond text. Consider a fraud investigation agent that reviews scanned ID documents and transaction logs, or a virtual financial advisor that can interpret charts and PDFs in addition to chat. Multi-modal AI refers to systems that can process and combine text, images, audio, and even other data like tables."}),"\n",(0,a.jsxs)(n.p,{children:["Today, there are early examples: OCR (optical character recognition) models to read documents, vision-language models (like document question-answering systems) that can take a document image and a query about it, etc. For retail banking, ",(0,a.jsx)(n.strong,{children:"document understanding is a key area"})," \u2013 from KYC verification documents to bank statements, a lot of information is locked in PDFs or images. Multi-modal models could automatically extract insights from a pile of paperwork or help a customer understand a chart from their spending history."]}),"\n",(0,a.jsx)(n.p,{children:"However, integrating multi-modal capabilities comes with constraints:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Multi-modal models (like those combining vision and language) are often large and still experimental. If you need a model that reads images and responds in text, you might be looking at very new research systems (with correspondingly uncertain behavior)."}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["An alternative is a pipeline: use a dedicated tool for the non-text part (e.g. an OCR engine or an image classifier) and then feed the results into a language model. This can work well (and is currently a common approach) but requires orchestration and can suffer from ",(0,a.jsx)(n.em,{children:"latency"})," or ",(0,a.jsx)(n.em,{children:"error propagation"})," between stages."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["There\u2019s also the question of ",(0,a.jsx)(n.strong,{children:"data"})," \u2013 training a truly integrated multi-modal model for your domain might not be feasible if you don\u2019t have a large corpus of paired image-text data (e.g. thousands of annotated loan application forms). Using a pre-trained multimodal foundation model is possible, but those are typically very large (and often closed-source)."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Regulatory clarity"})," on multi-modal outputs is evolving. For instance, if an AI analyses an image of an official ID, what audit trail is needed? Ensuring the visual data is handled with the same care as text data is important (e.g. no retention of images beyond their use, proper encryption, etc.)."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["Given these factors, the emerging recommendation for most in 2025 is to ",(0,a.jsx)(n.strong,{children:"start leveraging multi-modal AI in a cautious, incremental way"}),". You might begin by connecting a vision API to an LLM in a limited scope pilot \u2013 for example, allow a chatbot to retrieve a relevant figure from a chart image by calling an external vision service. Monitor the value and issues. Over the longer term (next 2\u20133 years), expect multi-modal foundation models to become more accessible and enterprise-friendly, at which point adopting one platform that natively handles text + documents could yield big advantages (Gartner analysts forecast a rapid rise in such use within five years5). The key is to ensure your architecture is ",(0,a.jsx)(n.strong,{children:"modular"})," enough that you can plug in these capabilities when ready \u2013 e.g. design your agent to call specialized modules for vision or speech when needed, rather than assuming all input is plain text."]}),"\n",(0,a.jsx)(n.p,{children:"In retail banking specifically, likely early wins for multi-modality will be:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Document ingestion and Q&A:"})," e.g. an agent that reads a PDF policy document and answers questions about it (combining document OCR and an LLM)."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Receipt or Invoice Processing:"})," a system that takes images of receipts or invoices and automatically categorizes expenses or flags discrepancies (useful for personal finance management tools)."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Visual Fraud Signals:"})," analyzing things like screenshot evidence of phishing or fake IDs \u2013 here a model might need to see an image and describe issues or inconsistencies."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Voice-enabled assistants:"})," integrating speech-to-text for voice banking assistants, though that\u2019s more about modality input/output than model internals (and can often be handled by separate ASR (automatic speech recognition) feeding a language model)."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Each of these can be approached with a combination of narrow models (for the vision or speech part) and language models. The long-term vision is a unified agent that seamlessly handles any modality. Banks should track progress but not rush to adopt multi-modal models until they are sufficiently robust for the given task, due to the added complexity."}),"\n",(0,a.jsx)(n.h3,{id:"cost-optimization-techniques",children:"Cost Optimization Techniques"}),"\n",(0,a.jsx)(n.p,{children:"Optimizing cost is not just about picking a cheaper model; it\u2019s an ongoing discipline in agentic system design. Some tactics to consider:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Right-sizing the Model:"})," Don\u2019t use a 175B-parameter model if a 7B model suffices for 95% of queries. Use evaluation to determine the smallest model that meets your accuracy requirements. Often, fine-tuning or RAG can allow use of a smaller base model. Also explore distillation \u2013 large models\u2019 knowledge can sometimes be distilled into smaller models for deployment, retaining much of the performance at a fraction of the runtime cost."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Dynamic Model Routing:"})," For heterogeneous workloads, you can route requests to different models based on complexity. For example, simple or routine questions go to a small, fast model, whereas only the tricky ones get escalated to a big LLM. Early research prototypes show that such smart routing can dramatically cut average cost while maintaining quality",(0,a.jsx)(n.a,{href:"https://openreview.net/pdf?id=pZFJLsIY2m#:~:text=integrate%20multiple%20components%2C%20often%20LLMs,2%20Instruct%20%281B",children:"openreview.net"}),(0,a.jsx)(n.a,{href:"https://openreview.net/pdf?id=pZFJLsIY2m#:~:text=generates%20100%20tokens%20in%200,weighted%20matching%20to",children:"openreview.net"}),". The key is having a reliable way to predict which queries need the \u201cpower\u201d of the large model \u2013 possibly via confidence scores or a lightweight classifier. As this technique matures, expect it to become a standard part of enterprise AI stacks (some vendors are already exploring cost-based routers)."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Caching and Reuse:"})," Many queries in banking are repetitive (think: \u201cWhat\u2019s my balance?\u201d or \u201cHow do I reset my password?\u201d). Caching LLM responses for common queries (where privacy allows) can save cost and improve latency. Even at a finer grain, caching vector retrieval results or intermediate computations can help. Just be cautious to invalidate caches when underlying data changes (e.g. don\u2019t cache a balance for too long). In internal agent use cases (like research assistants), you can cache aggressively since slight staleness might be acceptable."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Batching and Concurrency:"})," If using cloud API models, costs are often linear per call, so combining multiple tasks into one prompt can sometimes save money (if the API pricing is per token and you can utilize more tokens in one go). For instance, processing 10 small requests in a single batch prompt to a model might be cheaper than 10 separate calls \u2013 but this depends on the API and context window limits. For self-hosted models, increasing batch size improves throughput and amortizes overhead, up to hardware limits."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Monitoring and Usage Governance:"})," Keep an eye on usage patterns and have budgets or alerts. It\u2019s easy for costs to creep if an agent starts being used more widely or if prompts unintentionally grow in length. One Gartner report warned that if CIOs don\u2019t understand how GenAI costs scale, they could miscalculate budgets by 5-10\xd72. To avoid surprises, simulate worst-case volumes and track cost per user or per transaction as a KPI. Negotiate with vendors for volume discounts if applicable."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Tiered Model Serving:"})," Consider offering multiple tiers of service: a fast, minimal-cost option (perhaps with limited capabilities) and a premium thorough option using a larger model. For example, an automated email draft might first be generated by a local small model; only if the user requests a more polished version do you call a costly LLM. This \u201cfallback\u201d approach ensures you spend heavy compute only when needed. It pairs well with uncertainty detection \u2013 if the small model is unsure or produces low-confidence output, then escalate to the big model."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Efficient Implementation:"})," Low-level optimizations like quantizing model weights (using 8-bit or 4-bit precision) can greatly reduce memory and increase speed with negligible accuracy loss. This is especially valuable for self-hosted models \u2013 quantization and pruning might allow you to run an otherwise 20B model on commodity hardware. Similarly, utilize GPU inference optimizations or specialized AI hardware if you have it, to get more throughput per dollar."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["In summary, treat cost as a design constraint ",(0,a.jsx)(n.em,{children:"just like accuracy"}),". By architecting the system with cost in mind (e.g. modular design, caching, model choice) and continuously monitoring, you can often achieve ",(0,a.jsx)(n.strong,{children:"an order-of-magnitude cost reduction"})," without significantly sacrificing quality. In banking, where margins are slim and compliance demands can inflate cost (e.g. needing on-prem solutions), these levers make the difference between a sustainable AI product and one that gets shut down for being too expensive."]}),"\n",(0,a.jsx)(n.h3,{id:"risk-and-governance-considerations",children:"Risk and Governance Considerations"}),"\n",(0,a.jsx)(n.p,{children:"Deploying agentic AI in retail banking requires a robust risk mitigation strategy from day one. Key areas to address include:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Hallucination and Misinformation:"})," No model is 100% free of hallucination risk. Even with RAG and fine-tuning, there will be cases where the AI produces an incorrect statement confidently. In high-stakes use (like financial advice), this is unacceptable without safeguards. Strategies to manage this include: instructing the model to say \u201cI don\u2019t know\u201d or defer when unsure, building verification steps (e.g. cross-check important answers against a knowledge base or calculation), and limiting the scope of open-ended generation. Some studies have shown that combining techniques \u2013 retrieval, careful prompting, and reinforcement learning feedback \u2013 can reduce hallucination rates drastically (one 2024 experiment noted up to 96% reduction in certain settings by layering these4). While elimination of hallucination may not be feasible, ",(0,a.jsx)(n.strong,{children:"design for containment"}),": know what the model doesn\u2019t know, and have it fail gracefully."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Privacy and Data Security:"})," Bank data is sensitive by default. If using third-party models (API or SaaS), ensure no customer data is retained or used for provider training (some providers allow opting out of data retention). Apply encryption in transit and at rest. Anonymize inputs where possible (e.g. replace account numbers with a placeholder before sending to a model). On the flip side, if you deploy models on-premises with access to internal data, secure those models \u2013 they become new high-value targets. Limit which data the model sees based on user permissions (this is an often overlooked aspect: an AI agent should enforce the same data access controls as any software. For example, a customer service bot should not reveal another customer\u2019s data). Log all model queries and responses for audit, just as you would log human agent interactions in some scenarios."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Bias and Fairness:"})," Models can reflect or even amplify biases in data. A risk in banking is if an AI system gives different quality of service or suggestions to customers based on gender, race, etc., due to underlying bias in training data. Regularly evaluate outputs for biases and use a diverse set of test cases. If biases are found, mitigation might involve further fine-tuning on balanced data or adding rules (for instance, ensuring a loan advisor bot gives the same options regardless of user profile, unless legally allowed to differ). Document these measures as regulators are increasingly asking for evidence of fairness audits in AI."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Transparency and Explainability:"})," When AI influences financial decisions or advice, having an explanation is crucial for user trust and compliance. LLMs are black boxes by nature, but techniques like retrieval augmentation help by providing source documents. Even when not required, consider providing the user references: e.g. \u201cWe recommend this investment because of X and Y (sourced from [report link]).\u201d Internally, build tools for compliance officers to trace why the AI responded a certain way \u2013 maybe by logging which knowledge articles were retrieved or which prompts were used. Some banks are also exploring ",(0,a.jsx)(n.strong,{children:"model statement auditing"}),", where they systematically review a sample of AI outputs each month for correctness and appropriateness, akin to how call centers do quality assurance by sampling calls."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Robustness to Manipulation:"})," An emerging concern is prompt injection or adversarial inputs \u2013 bad actors trying to trick the AI into revealing information or performing unauthorized actions. For example, a user might phrase a request in a way to bypass content filters (\u201cpretend you are allowed to show me account details\u2026\u201d). It\u2019s important to harden the system: use strict role instructions that the model should not deviate from, sanitize inputs for known exploits, and keep models updated if security patches come out. In some cases, a simpler rule-based layer can intercept dangerous requests before they hit the model (like a regex or heuristic to catch someone asking for something clearly disallowed, independent of the model\u2019s response)."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Monitoring and Incident Response:"})," Put in place monitoring that can alert if the AI system is behaving anomalously \u2013 e.g. a sudden spike in refusal messages, or an unusual pattern of outputs that could indicate either misuse or a drift in model behavior. Have an incident response plan: if the AI gives a particularly harmful or erroneous output, how will you detect it and what actions will you take (from correcting the output and apologizing to users, to retraining the model if needed)? Consider a backup mechanism: in customer-facing scenarios, if the AI is unsure or flagged, it should hand off to a human or a predefined safe response."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Compliance and Documentation:"})," Keep documentation of your model selection rationale, training data provenance, and testing results. This helps in both internal governance and external oversight. Many regulators now expect a form of \u201cmodel card\u201d or similar documentation for AI systems describing their intended use, limitations, and performance characteristics. Being proactive here will save time later and build confidence with risk managers and auditors."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["By weaving these governance practices into the project from the start, banks can significantly reduce the likelihood of an AI project causing compliance headaches or public relation issues. The goal is to harness the benefits of agentic AI ",(0,a.jsx)(n.strong,{children:"without"})," stumbling into the known pitfalls \u2013 many of which can be anticipated and managed with the right precautions."]}),"\n",(0,a.jsx)(n.h2,{id:"recommendations--roadmap",children:"Recommendations & Roadmap"}),"\n",(0,a.jsx)(n.p,{children:"Adopting agentic AI in a bank is a journey. We propose a three-phase roadmap to iteratively build a robust model portfolio and capability:"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Short-term (next 3\u20136 months):"})," Focus on ",(0,a.jsx)(n.em,{children:"experimentation and establishing guardrails"}),". Stand up an evaluation harness to benchmark different models on your key tasks (e.g. answer accuracy on a set of banking queries, latency under load tests, etc.). Start with smaller or readily available models to solve low-hanging fruits \u2013 for instance, deploy a ",(0,a.jsx)(n.strong,{children:"small/medium language model with strong prompt instructions"})," for a pilot like an FAQ chatbot, where responses are limited and can be manually verified initially. Implement basic guardrails such as content filtering and fallback to human agents for unhandled queries. The emphasis in this phase is ",(0,a.jsx)(n.strong,{children:"control and understanding"}),": get a sense of model behavior, gather feedback, and ensure you have monitoring in place. Also, this is the time to address data preparation for future steps (e.g. begin curating a knowledge base for RAG, start collecting training examples from interactions). By the end of this phase, you should have confidence in evaluating AI outputs and a clear idea of where the model meets or falls short of requirements."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Mid-term (6\u201318 months):"})," Gradually ",(0,a.jsx)(n.em,{children:"increase sophistication while managing risk"}),". Based on early results, consider introducing ",(0,a.jsx)(n.strong,{children:"Retrieval-Augmented Generation"})," on a carefully curated internal knowledge base. For example, connect your chatbot or agent to a secure index of policy documents, product details, and guidelines, so it can provide up-to-date, compliant answers. This will likely improve accuracy and reduce hallucinations, but will require effort to build and govern the knowledge repository (ensure documents are approved and kept current). In parallel, identify narrow tasks that would benefit from ",(0,a.jsx)(n.strong,{children:"selective fine-tuning"}),". A good candidate might be a classification or routing task (like triaging fraud alerts or categorizing customer requests) where you have lots of labeled examples \u2013 a fine-tuned SLM could excel here and operate with low latency. Introduce such fine-tuned models as specialized components in your system (for instance, the fine-tuned model handles the classification and then hands off to an LLM for explanation). During this phase, also work on ",(0,a.jsx)(n.strong,{children:"integrating evaluation into regular operations"}),": for example, monthly metrics on accuracy and cost, and a review process for any incidents. The mid-term goal is to achieve ",(0,a.jsx)(n.strong,{children:"measurable improvements in performance"})," (more accurate answers, faster responses, reduced manual work) on key use cases, while still keeping a human in the loop for oversight on any critical decisions."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Long-term (18+ months, ongoing):"})," Evolve towards a ",(0,a.jsx)(n.em,{children:"portfolio of models and continuous optimization"}),". By this stage, you likely have multiple AI components \u2013 perhaps a large general model, a few fine-tuned specialist models, and a retrieval system, all orchestrated. Now focus on ",(0,a.jsx)(n.strong,{children:"portfolio optimization"}),": implement dynamic routing so that each query is handled by the most efficient model that can do the job (as discussed in cost optimization). This might involve developing a meta-controller that evaluates input complexity and chooses between a fast path or a slow path. Also consider model ",(0,a.jsx)(n.strong,{children:"distillation or compression"})," efforts: if your usage of a large model is high, investigate creating a distilled version or using techniques to run it cheaper (quantization, compilers, etc.). ",(0,a.jsx)(n.strong,{children:"Multi-modal expansion"})," can also come into play \u2013 start adding capabilities for images or other data if those use cases have proven value, possibly by incorporating new foundation models that support those modalities (which by then may be more mature). Simultaneously, institutionalize the ",(0,a.jsx)(n.strong,{children:"governance processes"}),": periodic re-training to combat drift, bias audits, cost audits, and an AI governance board review for new use cases. The long-term phase is about scaling the solution in a sustainable way: more use cases onboarded, broader acceptance by staff and customers, and tight integration with business workflows. Essentially, AI agents become a normal part of operations, with a robust infrastructure ensuring they remain ",(0,a.jsx)(n.strong,{children:"accurate, compliant, and cost-effective over time"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["Throughout all phases, maintain a pragmatic outlook. The field will continue to evolve quickly \u2013 new models or techniques (perhaps better at reasoning or offering transparency) will emerge. Be ready to pilot those, but do so within the framework of your decision model: assess their true added value against the criteria of capability, cost, and risk. Avoid chasing hype; instead, let the ",(0,a.jsx)(n.strong,{children:"measured improvements on your KPIs guide adoption"}),". By following the above roadmap, an organization can steadily increase its AI sophistication while controlling risks, rather than a big-bang approach that might fail to deliver lasting value."]}),"\n",(0,a.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,a.jsxs)(n.p,{children:["In the rush to embrace AI, it\u2019s easy to grab whatever powerful model is available and bolt it onto a problem. This paper argues for a more disciplined, framework-driven approach \u2013 especially in a sensitive domain like retail banking. ",(0,a.jsx)(n.strong,{children:"Selecting the right AI model (or combination of models) is foundational to building reliable and scalable agentic systems."})," We\u2019ve outlined how to break down this selection: understand the taxonomy of model types and their trade-offs, choose an adaptation strategy aligned with your data and needs, and rigorously evaluate on multiple axes beyond just accuracy."]}),"\n",(0,a.jsx)(n.p,{children:"By applying this framework, architects and product leaders can make evidence-based decisions instead of guesswork. For example, they might recognize that a fine-tuned small model with retrieval augmentation offers the best balance for a given use case \u2013 providing accuracy with low latency and cost \u2013 whereas another task truly needs a cutting-edge LLM and justifies its expense. The decision flow and criteria we presented help navigate these choices systematically, ensuring factors like domain specificity, data availability, latency requirements, and compliance are all weighed."}),"\n",(0,a.jsxs)(n.p,{children:["Ultimately, building sustainable AI agents is an ",(0,a.jsx)(n.strong,{children:"iterative journey of adaptation"}),". Start with straightforward solutions and add complexity only as needed. Use the short/mid/long-term roadmap to gradually layer capabilities, all the while monitoring outcomes. And remain flexible: as new techniques (perhaps better multi-modal models or safer training methods) become available, incorporate them if they fit your framework and improve the goals of capability, cost, or risk."]}),"\n",(0,a.jsxs)(n.p,{children:["Banking has long been a data-driven industry, and the promise of agentic AI is immense \u2013 from democratizing financial knowledge for customers to augmenting employees with intelligent assistants. Realizing this promise at scale will require not just clever models, but ",(0,a.jsx)(n.strong,{children:"wise model selection and governance"}),". By treating model choice as a strategic decision and continuously aligning it with use case needs, organizations can harness AI\u2019s power responsibly. In doing so, they set the stage for AI systems that deliver high impact ",(0,a.jsx)(n.em,{children:"and"})," high trust \u2013 a combination that will define the winners in the next era of digital banking."]}),"\n",(0,a.jsx)(n.h2,{id:"footnotes",children:"Footnotes"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Gartner Press Release (July 29, 2024). ",(0,a.jsx)(n.em,{children:'"30% of Generative AI Projects Will Be Abandoned After Proof of Concept by End of 2025."'})," Gartner cites poor data quality, inadequate risk controls, rising costs and unclear value as top reasons for failure",(0,a.jsx)(n.a,{href:"https://www.gartner.com/en/newsroom/press-releases/2024-07-29-gartner-predicts-30-percent-of-generative-ai-projects-will-be-abandoned-after-proof-of-concept-by-end-of-2025#:~:text=At%20least%2030,value%2C%20according%20to%20Gartner%2C%20Inc",children:"gartner.com"}),". \u21a9 \u21a92"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Gartner Press Release (Oct 21, 2024). ",(0,a.jsx)(n.em,{children:'"Four Emerging Challenges to Delivering Value from AI Safely and at Scale."'})," A survey of CIOs found over 90% concerned with AI costs; Gartner notes ",(0,a.jsx)(n.em,{children:"\u201ccost is as big an AI risk as security or hallucinations.\u201d"}),(0,a.jsx)(n.a,{href:"https://www.gartner.com/en/newsroom/press-releases/2024-10-21-gartner-identifies-four-emerging-challenges-to-delivering-value-from-ai-safely-and-at-scale#:~:text=The%20Cost%20of%20AI%20Can,risk%20as%20security%20or%20hallucinations",children:"gartner.com"})," \u21a9 \u21a92"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["F. Wang et al. (2025). ",(0,a.jsx)(n.em,{children:"A Survey on Collaborating Small and Large Language Models."})," (arXiv 2510.13890). Highlights that while LLMs have advanced capabilities, they incur high fine-tuning and inference costs and latency, whereas SLMs offer efficiency and edge deployability",(0,a.jsx)(n.a,{href:"https://www.arxiv.org/abs/2510.13890#:~:text=,a%20systematic%20survey%20of%20SLM",children:"arxiv.org"}),". Industry data suggests training frontier LLMs can exceed $100M, and inference cost-per-query can be 100\xd7 higher than for smaller models",(0,a.jsx)(n.a,{href:"https://labelyourdata.com/articles/llm-fine-tuning/slm-vs-llm#:~:text=5,million%20queries%20by%20over%20100x",children:"labelyourdata.com"}),". \u21a9 \u21a92 \u21a93 \u21a94 \u21a95"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["A. Bora & H. Cuay\xe1huitl (2024). ",(0,a.jsx)(n.em,{children:"\u201cSystematic Analysis of RAG-Based LLMs for Medical Chatbots.\u201d"})," (Machine Learning & Knowledge Extraction). Demonstrates that Retrieval-Augmented Generation improves factual accuracy and reduces hallucinations: ",(0,a.jsx)(n.em,{children:"\u201cRAG effectively reduces the problem of generating factually incorrect content.\u201d"}),(0,a.jsx)(n.a,{href:"https://www.mdpi.com/2504-4990/6/4/116#:~:text=technique%20used%20for%20enhancing%20the,then%20encoded%20into%20a%20vectorised",children:"mdpi.com"})," Also notes combining RAG with fine-tuning yielded best results in domain QA tasks",(0,a.jsx)(n.a,{href:"https://www.mdpi.com/2504-4990/6/4/116#:~:text=,are%20key%20for%20best%20results",children:"mdpi.com"}),". \u21a9 \u21a92 \u21a93"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Gartner Press Release (Sept 9, 2024). ",(0,a.jsx)(n.em,{children:'"40% of GenAI Solutions Will Be Multimodal by 2027."'})," Predicts a rapid shift towards multi-modal AI (text, image, audio) in the next few years",(0,a.jsx)(n.a,{href:"https://www.gartner.com/en/newsroom/press-releases/2024-09-09-gartner-predicts-40-percent-of-generative-ai-solutions-will-be-multimodal-by-2027#:~:text=9",children:"gartner.com"}),", from a baseline of only 1% in 2023. Highlights that native multimodal training will unlock new AI capabilities across industries. \u21a9 \u21a92"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["R. Bommasani, P. Liang ",(0,a.jsx)(n.em,{children:"et al."})," (2023). ",(0,a.jsx)(n.em,{children:"Holistic Evaluation of Language Models (HELM)."})," Presents a multi-metric evaluation framework covering accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency for language models",(0,a.jsx)(n.a,{href:"https://ar5iv.labs.arxiv.org/html/2211.09110#:~:text=potential%20scenarios%20%28i,models%20and%20metrics%20are%20clearly",children:"ar5iv.labs.arxiv.org"}),". Emphasizes looking beyond accuracy to trade-offs across these metrics. \u21a9"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Y. Zhu & D. Kang (2025). ",(0,a.jsx)(n.em,{children:"\u201cHuman Data is (Probably) More Expensive Than Compute for Training Frontier LLMs.\u201d"})," Analysis indicating that the expense of human-labeled data for fine-tuning now exceeds marginal compute costs by a factor of ~3.1\xd7 for cutting-edge models",(0,a.jsx)(n.a,{href:"https://medium.com/@danieldkang/human-data-is-probably-more-expensive-than-compute-for-training-frontier-llms-3c916ef309e4#:~:text=We%20then%20calculate%20the%20sum,art%20AI%20models",children:"medium.com"}),". Case studies showed data labeling costs outpacing training costs (e.g. $60k in annotations vs $360 compute in one example)",(0,a.jsx)(n.a,{href:"https://medium.com/@danieldkang/human-data-is-probably-more-expensive-than-compute-for-training-frontier-llms-3c916ef309e4#:~:text=If%20we%20estimate%20that%20a,marginal%20compute%20cost%20for%20training",children:"medium.com"}),", underscoring the importance of data in model adaptation efforts. \u21a9 \u21a92"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>r});var t=i(6540);const a={},s=t.createContext(a);function o(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);